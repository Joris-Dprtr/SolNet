{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Set up the notebook before running multiple instances\n",
    "# Figure 3: All false\n",
    "# Figure 4: weather_variables = True\n",
    "# Table 1: seasonal = True\n",
    "# Figure 5.a: distance = True\n",
    "# Figure 5.b: distance = True, weather_variables = True\n",
    "weather_variables = False\n",
    "distance = True\n",
    "seasonal = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:27.854700300Z",
     "start_time": "2024-12-16T16:04:27.831976400Z"
    }
   },
   "id": "e8e9c67f49fb800a"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "system_id = 0\n",
    "kilometers = 0\n",
    "year = 2021\n",
    "month = 'jan'\n",
    "months = {'jan':0,'feb':1, 'mar':2, 'apr':3,'may':4,'jun':5,'jul':6,'aug':7,'sep':8,'oct':'9','nov':10,'dec':11}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:29.009954Z",
     "start_time": "2024-12-16T16:04:28.979645Z"
    }
   },
   "id": "49eef3da1999505"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:29.328011800Z",
     "start_time": "2024-12-16T16:04:29.311680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base library imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SolNet imports\n",
    "from src.util import formulas as fm\n",
    "from src.data.datafetcher import PvFetcher\n",
    "from src.data.featurisation import Featurisation\n",
    "from src.util.open_meteo_api import Open_meteo\n",
    "from src.util.dwd_data_prep import Weather_data\n",
    "from src.tensors.tensorisation import Tensors\n",
    "from src.models.lstm import LSTM\n",
    "from src.models.training import Training\n",
    "from src.models.training import save_model\n",
    "from src.evaluation.evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Hyperparameters needed for a run:\n",
    "\n",
    "# Data fetching\n",
    "locations_used = 1\n",
    "start_date = 2005\n",
    "end_date = 2018\n",
    "dwd_icon_vars = ['relhum_2m', 'aswdifd_s', 'aswdir_s', 't_2m', 'ww', 'clct']\n",
    "open_meteo_names = ['relative_humidity_2m','diffuse_radiation', 'direct_radiation', 'temperature_2m', 'weather_code', 'cloud_cover']\n",
    "dwd_path = \"../data/weather/dwd_huggingface/\"\n",
    "\n",
    "# Forecasting parameters\n",
    "target = 'P'\n",
    "past_features = ['P']\n",
    "if weather_variables is True:\n",
    "    future_features = ['hour_sin', 'hour_cos','relative_humidity_2m','diffuse_radiation', 'direct_radiation']\n",
    "else:\n",
    "    future_features = ['hour_sin','hour_cos']\n",
    "lags = 24\n",
    "forecast_period = 24\n",
    "gap = 0 \n",
    "forecast_gap = 0\n",
    "\n",
    "# Lstm parameters\n",
    "hidden_size = 400\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:29.734269800Z",
     "start_time": "2024-12-16T16:04:29.710951600Z"
    }
   },
   "id": "194e4f3040adb64d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Target location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed10c4c3f4d379e"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Get the list of ID codes\n",
    "id_list = pd.read_csv('../data/netherlands/id_list.csv',header=None)\n",
    "\n",
    "# We need the meta data for the source location\n",
    "meta_data_nl = pd.read_csv('../data/netherlands/installations Netherlands.csv', delimiter=';')\n",
    "\n",
    "# Decide on the location\n",
    "installation = meta_data_nl.index[meta_data_nl['id'] == id_list.loc[system_id].values[0]][0]\n",
    "site_id = meta_data_nl.iloc[installation]['id']\n",
    "latitude = meta_data_nl.iloc[installation]['Latitude']\n",
    "longitude = meta_data_nl.iloc[installation]['Longitude']\n",
    "peak_power = meta_data_nl.iloc[installation]['Watt Peak']/1000\n",
    "tilt = meta_data_nl.iloc[installation]['Tilt']\n",
    "azimuth = meta_data_nl.iloc[installation]['Orientation']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:30.881798100Z",
     "start_time": "2024-12-16T16:04:30.864438600Z"
    }
   },
   "id": "879cf97881ff0aa1"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# For data mis-specification\n",
    "if distance: \n",
    "    bearing_degrees = 125\n",
    "    latitude, longitude = fm.calculate_new_position(latitude, longitude, kilometers, bearing_degrees)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:32.334735100Z",
     "start_time": "2024-12-16T16:04:32.310921900Z"
    }
   },
   "id": "b3bb664252a5ef5c"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "'distance_amstelveen_0_0km'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique name for the data, model and metrics\n",
    "if distance and weather_variables:\n",
    "    data_name = 'distance&weather_' + meta_data_nl.iloc[installation]['Plaats'].lower() + '_' + str(installation) + '_' + str(kilometers) + 'km'\n",
    "elif distance:\n",
    "    data_name = 'distance_' + meta_data_nl.iloc[installation]['Plaats'].lower() + '_' + str(installation) + '_' + str(kilometers) + 'km'\n",
    "elif weather_variables:\n",
    "    data_name = 'weather_' + meta_data_nl.iloc[installation]['Plaats'].lower() + '_' + str(installation)\n",
    "elif seasonal:\n",
    "    data_name = 'seasonal_' + meta_data_nl.iloc[installation]['Plaats'].lower() + '_' + str(installation) + '_' + month + '_' + str(year)\n",
    "else:\n",
    "    data_name = 'base_' + meta_data_nl.iloc[installation]['Plaats'].lower() + '_' + str(installation)\n",
    "\n",
    "data_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:33.518770300Z",
     "start_time": "2024-12-16T16:04:33.492298500Z"
    }
   },
   "id": "6b98fe610c8948df"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Create the folders to save the data and models\n",
    "data_folder = '../results/NL/'\n",
    "model_folder = '../models/NL/' + data_name\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:34.251838800Z",
     "start_time": "2024-12-16T16:04:34.220482900Z"
    }
   },
   "id": "d86a576bc67d4a1"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "data_nl = pd.read_parquet('../data/netherlands/production.parquet', engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:35.486296400Z",
     "start_time": "2024-12-16T16:04:34.863837100Z"
    }
   },
   "id": "a59eb543a875f95d"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(Timestamp('2020-01-01 00:00:00'), Timestamp('2021-08-08 23:00:00'))"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nl = data_nl.loc[site_id]\n",
    "data_nl = data_nl.resample('H').mean()\n",
    "data_nl = data_nl.rename(columns={\"watt\":\"P\"})\n",
    "\n",
    "# Remove everything up until the first entry starting at midnight\n",
    "if weather_variables or distance:\n",
    "    first_index = data_nl[(data_nl.index.hour == 0) & (data_nl.index.day == 1) & (data_nl.index.month == 1) & (data_nl.index.year == 2020)].index[0]\n",
    "else:\n",
    "    first_index = data_nl[(data_nl.index.hour == 0) & (data_nl.index.day == 1) & (data_nl.index.year == 2019)].index[0]\n",
    "# Remove everything after the last entry for the final day of december in the data\n",
    "if seasonal:\n",
    "    last_index = data_nl[(data_nl.index.hour == 23) & (data_nl.index.month == int(months[month])+1) & (data_nl.index.year == year)].index[-1]\n",
    "elif weather_variables or distance:\n",
    "    last_index = data_nl[(data_nl.index.hour == 23)].index[-1]    \n",
    "else:\n",
    "    last_index = data_nl[(data_nl.index.hour == 23) & (data_nl.index.month == 2)].index[-1]\n",
    "target_data = data_nl.loc[first_index:last_index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:04:35.537013900Z",
     "start_time": "2024-12-16T16:04:35.489309700Z"
    }
   },
   "id": "c19895e1ca9836a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Source location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baaa56493597bd4a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from base location...\n"
     ]
    }
   ],
   "source": [
    "# Fetch data from PVGIS\n",
    "data_PVGIS = PvFetcher(latitude,longitude,peak_power, tilt, azimuth, locations=locations_used, start_date=start_date, end_date=end_date,optimal_angles=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.846455300Z",
     "start_time": "2024-12-16T13:39:28.266507500Z"
    }
   },
   "id": "30957dec88587bd9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "data = [data_PVGIS.dataset[0]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.851779100Z",
     "start_time": "2024-12-16T13:39:35.846455300Z"
    }
   },
   "id": "a8022fa6f06fbdb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Featurisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0927a4cbed4240"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1ac8eb1137604f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53875ac2a81ecd0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Decide on the features to use in making the model (Note that 'P' should always be included since it's the target variable)\n",
    "dataset = Featurisation(data).base_features(past_features)\n",
    "\n",
    "# Use cyclic features as well\n",
    "dataset = Featurisation(dataset).cyclic_features(yearly=True)\n",
    "features = dataset[0].columns # update the features\n",
    "source_data = dataset[0].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.886071500Z",
     "start_time": "2024-12-16T13:39:35.851779100Z"
    }
   },
   "id": "2f205764b73ce294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weather features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ed0a5edb290ce5"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if weather_variables is True:\n",
    "    \n",
    "    start = dataset[0].index[0].date()\n",
    "    end = dataset[0].index[-1].date()\n",
    "    target_end = target_data.index[-1].date()\n",
    "    weather_fetcher = Open_meteo(latitude, longitude, open_meteo_names, start, target_end)\n",
    "    \n",
    "    weather_data = weather_fetcher.get_open_meteo_hourly()\n",
    "    weather_data.set_index('date', inplace=True)\n",
    "    weather_data = weather_data.tz_localize(None)\n",
    "    \n",
    "    source_data = pd.concat([dataset[0], weather_data], axis=1)\n",
    "    source_data = source_data.loc[start:end]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.892257100Z",
     "start_time": "2024-12-16T13:39:35.886071500Z"
    }
   },
   "id": "b3b9ecb08f712cad"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                       P  hour_sin  hour_cos  month_sin  month_cos\ntime                                                              \n2005-01-01 00:00:00  0.0  0.000000  1.000000        0.5   0.866025\n2005-01-01 01:00:00  0.0  0.258819  0.965926        0.5   0.866025\n2005-01-01 02:00:00  0.0  0.500000  0.866025        0.5   0.866025\n2005-01-01 03:00:00  0.0  0.707107  0.707107        0.5   0.866025\n2005-01-01 04:00:00  0.0  0.866025  0.500000        0.5   0.866025",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>hour_sin</th>\n      <th>hour_cos</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2005-01-01 00:00:00</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n    </tr>\n    <tr>\n      <th>2005-01-01 01:00:00</th>\n      <td>0.0</td>\n      <td>0.258819</td>\n      <td>0.965926</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n    </tr>\n    <tr>\n      <th>2005-01-01 02:00:00</th>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.866025</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n    </tr>\n    <tr>\n      <th>2005-01-01 03:00:00</th>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.707107</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n    </tr>\n    <tr>\n      <th>2005-01-01 04:00:00</th>\n      <td>0.0</td>\n      <td>0.866025</td>\n      <td>0.500000</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.907401300Z",
     "start_time": "2024-12-16T13:39:35.892257100Z"
    }
   },
   "id": "e79f62f95db6236e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f3f7a9d43b9fd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da22dbc59d930a02"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "target_featurisation = Featurisation([target_data])\n",
    "target_data = target_featurisation.cyclic_features()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.919851400Z",
     "start_time": "2024-12-16T13:39:35.907401300Z"
    }
   },
   "id": "15f13f0391f4a6fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weather features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f8c7dd9502df0ec"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if weather_variables is True:\n",
    "    \n",
    "    # Consolidate all the weather data in the individual folders to a single dataframe\n",
    "    weather_data_target = Weather_data(dwd_path, latitude, longitude, dwd_icon_vars).get_weather_data()\n",
    "    \n",
    "    # temperature is in Kelvin, transform to celsius to compare with OpenMeteo\n",
    "    if 't_2m' in dwd_icon_vars:\n",
    "        weather_data_target.t_2m = weather_data_target.t_2m - 272.15\n",
    "        \n",
    "    target_data = pd.concat([target_data, weather_data_target], axis=1)\n",
    "    target_data = target_data.loc[:target_end]\n",
    "    target_data.rename(columns=dict(zip(dwd_icon_vars, open_meteo_names)), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:35.919851400Z",
     "start_time": "2024-12-16T13:39:35.914220600Z"
    }
   },
   "id": "6e8b52316c080604"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Include domain knowledge into the target domain for scaling purposes\n",
    "# We know that the minimum power is always 0\n",
    "domain_min = [0.0]\n",
    "# We are going to assume that the maximum is the peak rated power times some degradation factor. In the paper we assume this degradation is 14%, this is the number also used by PVGIS\n",
    "# cf. https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/getting-started-pvgis/pvgis-user-manual_en#ref-9-hourly-solar-radiation-and-pv-data\n",
    "domain_max = [peak_power*0.86*1000]\n",
    "\n",
    "# For other features we just assume that the minimum and maximum are what we have seen in the source data, this data is freely available, so this is not a stretch\n",
    "other_features = past_features[1:] + future_features\n",
    "\n",
    "for i in range(len(other_features)):\n",
    "    domain_min.append(min(source_data[other_features[i]]))\n",
    "    domain_max.append(max(source_data[other_features[i]]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:36.004810200Z",
     "start_time": "2024-12-16T13:39:35.919851400Z"
    }
   },
   "id": "442eef9d63a6aad9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                       P  hour_sin  hour_cos  month_sin  month_cos\ntimestamp                                                         \n2019-10-01 00:00:00  0.0  0.000000  1.000000  -0.866025        0.5\n2019-10-01 01:00:00  0.0  0.258819  0.965926  -0.866025        0.5\n2019-10-01 02:00:00  0.0  0.500000  0.866025  -0.866025        0.5\n2019-10-01 03:00:00  0.0  0.707107  0.707107  -0.866025        0.5\n2019-10-01 04:00:00  0.0  0.866025  0.500000  -0.866025        0.5\n...                  ...       ...       ...        ...        ...\n2021-08-08 19:00:00  0.0 -0.965926  0.258819  -0.866025       -0.5\n2021-08-08 20:00:00  0.0 -0.866025  0.500000  -0.866025       -0.5\n2021-08-08 21:00:00  0.0 -0.707107  0.707107  -0.866025       -0.5\n2021-08-08 22:00:00  0.0 -0.500000  0.866025  -0.866025       -0.5\n2021-08-08 23:00:00  0.0 -0.258819  0.965926  -0.866025       -0.5\n\n[16272 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>hour_sin</th>\n      <th>hour_cos</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-01 00:00:00</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2019-10-01 01:00:00</th>\n      <td>0.0</td>\n      <td>0.258819</td>\n      <td>0.965926</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2019-10-01 02:00:00</th>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.866025</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2019-10-01 03:00:00</th>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.707107</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2019-10-01 04:00:00</th>\n      <td>0.0</td>\n      <td>0.866025</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-08-08 19:00:00</th>\n      <td>0.0</td>\n      <td>-0.965926</td>\n      <td>0.258819</td>\n      <td>-0.866025</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>2021-08-08 20:00:00</th>\n      <td>0.0</td>\n      <td>-0.866025</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>2021-08-08 21:00:00</th>\n      <td>0.0</td>\n      <td>-0.707107</td>\n      <td>0.707107</td>\n      <td>-0.866025</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>2021-08-08 22:00:00</th>\n      <td>0.0</td>\n      <td>-0.500000</td>\n      <td>0.866025</td>\n      <td>-0.866025</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>2021-08-08 23:00:00</th>\n      <td>0.0</td>\n      <td>-0.258819</td>\n      <td>0.965926</td>\n      <td>-0.866025</td>\n      <td>-0.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>16272 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:36.020435Z",
     "start_time": "2024-12-16T13:39:35.954775600Z"
    }
   },
   "id": "efe224b714fd5272"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33755eed0dac848c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f798611c3d20350"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4090, 24, 3]),\n torch.Size([1022, 24, 3]),\n torch.Size([4090, 24]),\n torch.Size([1022, 24]))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data in the torch.tensor format\n",
    "src_tensors = Tensors(source_data, 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap)\n",
    "\n",
    "# Split the data into train and test sets with separate tensors for features (X) and the target (y)\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = src_tensors.create_tensor()\n",
    "X_train_src.shape, X_test_src.shape, y_train_src.shape, y_test_src.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:36.036059300Z",
     "start_time": "2024-12-16T13:39:35.967639Z"
    }
   },
   "id": "34351506bdbfbd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5659b927ee23db17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the target dataset we require a separate \"evaluation set\" of a full year, apart from the train and test set. This makes the tensorisation of the data a bit more complex than what we did for the source domain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a078857f88e83d06"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Take apart the train and test data\n",
    "target_excl_eval = target_data[:-366*24]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:38.822170300Z",
     "start_time": "2024-12-16T13:39:38.807128Z"
    }
   },
   "id": "79aaf7cd590d4383"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Get the months we have available for training. We need this info to make separate cases for each unique case of having \"X months\" of data in the target domain\n",
    "training_months = list(target_excl_eval.index.month.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:39.027798200Z",
     "start_time": "2024-12-16T13:39:39.013624900Z"
    }
   },
   "id": "9aa969d15791db38"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# the timestamps of the training start points for each case of having \"X months\" of data\n",
    "train_starts = []\n",
    "for i in range(len(training_months)):\n",
    "    train_start = target_excl_eval[(target_excl_eval.index.month ==training_months[i])].index[0]\n",
    "    train_starts.append(train_start)\n",
    "    \n",
    "train_starts = list(reversed(train_starts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:39.340497600Z",
     "start_time": "2024-12-16T13:39:39.324346400Z"
    }
   },
   "id": "9a38ed6bf8227881"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 24, 3]) torch.Size([1, 24, 3]) torch.Size([364, 24, 3]) torch.Size([6, 24]) torch.Size([1, 24]) torch.Size([364, 24])\n",
      "torch.Size([30, 24, 3]) torch.Size([8, 24, 3]) torch.Size([364, 24, 3]) torch.Size([30, 24]) torch.Size([8, 24]) torch.Size([364, 24])\n",
      "torch.Size([54, 24, 3]) torch.Size([14, 24, 3]) torch.Size([364, 24, 3]) torch.Size([54, 24]) torch.Size([14, 24]) torch.Size([364, 24])\n",
      "torch.Size([79, 24, 3]) torch.Size([20, 24, 3]) torch.Size([364, 24, 3]) torch.Size([79, 24]) torch.Size([20, 24]) torch.Size([364, 24])\n",
      "torch.Size([103, 24, 3]) torch.Size([26, 24, 3]) torch.Size([364, 24, 3]) torch.Size([103, 24]) torch.Size([26, 24]) torch.Size([364, 24])\n",
      "torch.Size([128, 24, 3]) torch.Size([32, 24, 3]) torch.Size([364, 24, 3]) torch.Size([128, 24]) torch.Size([32, 24]) torch.Size([364, 24])\n",
      "torch.Size([151, 24, 3]) torch.Size([38, 24, 3]) torch.Size([364, 24, 3]) torch.Size([151, 24]) torch.Size([38, 24]) torch.Size([364, 24])\n",
      "torch.Size([176, 24, 3]) torch.Size([44, 24, 3]) torch.Size([364, 24, 3]) torch.Size([176, 24]) torch.Size([44, 24]) torch.Size([364, 24])\n",
      "torch.Size([201, 24, 3]) torch.Size([50, 24, 3]) torch.Size([364, 24, 3]) torch.Size([201, 24]) torch.Size([50, 24]) torch.Size([364, 24])\n",
      "torch.Size([225, 24, 3]) torch.Size([56, 24, 3]) torch.Size([364, 24, 3]) torch.Size([225, 24]) torch.Size([56, 24]) torch.Size([364, 24])\n",
      "torch.Size([250, 24, 3]) torch.Size([62, 24, 3]) torch.Size([364, 24, 3]) torch.Size([250, 24]) torch.Size([62, 24]) torch.Size([364, 24])\n"
     ]
    }
   ],
   "source": [
    "# Get the target data in lists holding all the tensors for each of the \"X months\" cases. This time with a train and test set, as well as a separate evaluation set. \n",
    "X_train_target_list = []\n",
    "X_test_target_list = []\n",
    "X_eval_target_list = []\n",
    "y_train_target_list = []\n",
    "y_test_target_list = []\n",
    "y_eval_target_list = []\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    tgt_tensors = Tensors(target_data.loc[train_starts[i]:], 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap, evaluation_length=24*365, domain_min=domain_min, domain_max=domain_max)\n",
    "    X_train_tgt, X_test_tgt, X_eval_tgt, y_train_tgt, y_test_tgt, y_eval_tgt = tgt_tensors.create_tensor()\n",
    "    X_train_target_list.append(X_train_tgt)\n",
    "    X_test_target_list.append(X_test_tgt)\n",
    "    X_eval_target_list.append(X_eval_tgt)\n",
    "    y_train_target_list.append(y_train_tgt)\n",
    "    y_test_target_list.append(y_test_tgt)\n",
    "    y_eval_target_list.append(y_eval_tgt) \n",
    "    print(X_train_tgt.shape, X_test_tgt.shape, X_eval_tgt.shape, y_train_tgt.shape, y_test_tgt.shape, y_eval_tgt.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T13:39:39.823232500Z",
     "start_time": "2024-12-16T13:39:39.791462400Z"
    }
   },
   "id": "3cdbbb4f080be078"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Source model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3786957ed4b4b06b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "my_lstm = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "my_lstm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5bf308c1fc0b7bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "training = Training(my_lstm, X_train_src, y_train_src, X_test_src, y_test_src, epochs,batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model and return the trained parameters and the best iteration\n",
    "state_dict_list, best_epoch = training.fit()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17feb56dd0a55ab0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the state dictionary of the best performing model\n",
    "my_lstm.load_state_dict(state_dict_list[best_epoch])\n",
    "\n",
    "# Save the model state dictionary for later use \n",
    "save_model(my_lstm, 'NL/' + data_name + '/model_' + data_name + '_transfer_0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78309de0faa97b09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Forecast with the model\n",
    "forecasts = my_lstm(X_test_src.to(device))\n",
    "\n",
    "# Evaluate the model performance\n",
    "source_eval = Evaluation(y_test_src.detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc2960f198f7417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Target model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed943c74f35620"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "# Create empty models for each of the periods\n",
    "target_lstm_list = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    target_lstm_list.append(LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device))\n",
    "    \n",
    "torch.save(target_lstm_list[0].state_dict(), '../models/NL/' + data_name + '/model_' + data_name + '_target_0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172a0306b3cfa853"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Initialize the trainer\n",
    "    training = Training(target_lstm_list[i+1], X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    target_lstm_list[i+1].load_state_dict(state_dict_list[best_epoch])\n",
    "    target_best_epochs.append(best_epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a707dd96e1a14b7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_RMSEs = []\n",
    "target_MBEs = []\n",
    "target_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "forecasts = target_lstm_list[0](X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "target_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = target_lstm_list[i+1](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    target_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0660e76f1cde60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Transfer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1aa75c7d725d84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Layers to freeze\n",
    "\n",
    "freezing = []\n",
    "\n",
    "for name, _ in my_lstm.lstm.named_parameters():\n",
    "    freezing.append(name)\n",
    "    \n",
    "freezing = freezing[:4]\n",
    "freezing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a1ee9bd9bff6639"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transfer_models = []\n",
    "transfer_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    transfer_model  = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "    transfer_model.load_state_dict(torch.load('../models/NL/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "       \n",
    "    for name, param in transfer_model.lstm.named_parameters():\n",
    "        if any(freezing_name in name for freezing_name in freezing):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Initialize the trainer\n",
    "    training = Training(transfer_model, \n",
    "                              X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], \n",
    "                              epochs=1000, batch_size = batch_size, learning_rate =learning_rate/100)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    transfer_model.load_state_dict(state_dict_list[best_epoch])\n",
    "    \n",
    "    transfer_best_epochs.append(best_epoch)\n",
    "    transfer_models.append(transfer_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "350c3b23d316dd6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transfer_RMSEs = []\n",
    "transfer_MBEs = []\n",
    "transfer_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "\n",
    "transfer_model = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "transfer_model.load_state_dict(torch.load('../models/NL/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "\n",
    "forecasts = transfer_model(X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = transfer_models[i](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5a423788b3e209"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa98825b51380"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_RMSEs = []\n",
    "baseline_MBEs = []\n",
    "baseline_MAEs = []\n",
    "\n",
    "# Evaluate a clean model, our forecast in this case is basically the first feature in our features tensor, as we predict the next day to be the previous one \n",
    "forecasts = X_eval_target_list[0][:,:,0]\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = X_eval_target_list[i][:,:,0]\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9a67df5080bb4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Final visualisation and export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19632e7bcce954a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(target_RMSEs,label='target')\n",
    "plt.plot(transfer_RMSEs,label='transfer')\n",
    "plt.plot(baseline_RMSEs, label='baseline')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a796e9ba899736a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_names = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    column_names.append(str(i) + 'm')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f738ffd4e8f96d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_metrics = pd.DataFrame([baseline_RMSEs, target_RMSEs, transfer_RMSEs,\n",
    "                            baseline_MBEs, target_MBEs, transfer_MBEs,\n",
    "                            baseline_MAEs, target_MAEs, transfer_MAEs, \n",
    "                            target_best_epochs, transfer_best_epochs],\n",
    "                           columns=column_names, index=['Baseline RMSE', 'Target RMSE', 'Transfer RMSE', \n",
    "                                                        'Baseline MBE', 'Target MBE', 'Transfer MBE', \n",
    "                                                        'Baseline MAE', 'Target MAE', 'Transfer MAE', \n",
    "                                                        'Target epoch', 'Transfer epoch']).transpose()\n",
    "\n",
    "all_metrics['Target epoch'] = all_metrics['Target epoch'].astype(int)\n",
    "all_metrics['Transfer epoch'] = all_metrics['Transfer epoch'].astype(int)\n",
    "all_metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cc8e6ba8e067e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if distance and weather_variables:\n",
    "    all_metrics.to_csv('../results/NL/' + 'summary_table_' + data_name + '.csv')\n",
    "elif distance:\n",
    "    all_metrics.to_csv('../results/NL/' + 'summary_table_' + data_name + '.csv')\n",
    "elif seasonal:\n",
    "    all_metrics.to_csv('../results/NL/' + 'summary_table_' + data_name + '.csv')\n",
    "elif weather_variables:\n",
    "    all_metrics.to_csv('../results/NL/' + 'summary_table_' + data_name + '.csv')\n",
    "else:\n",
    "    all_metrics.to_csv('../results/NL/' + 'summary_table_' + data_name + '.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e911dfa55c19f92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62ad32304b2e2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d445b499f51669a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
