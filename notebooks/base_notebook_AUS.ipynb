{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# THIS PARAMETER IS USED FOR LOOPING OVER THE NOTEBOOK IN THE \"run_all_customers\" NOTEBOOK\n",
    "system_id = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49eef3da1999505"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base library imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SolNet imports\n",
    "from src.data.datafetcher import PvFetcher\n",
    "from src.data.featurisation import Featurisation\n",
    "from src.tensors.tensorisation import Tensors\n",
    "from src.models.lstm import LSTM\n",
    "from src.models.training import Training\n",
    "from src.models.training import save_model\n",
    "from src.evaluation.evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameters needed for a run:\n",
    "\n",
    "# Data fetching\n",
    "locations_used = 1\n",
    "start_date = 2005\n",
    "end_date = 2010\n",
    "\n",
    "# Forecasting parameters\n",
    "target = 'P'\n",
    "past_features = ['P']\n",
    "future_features = ['hour_sin','hour_cos']\n",
    "lags = 24\n",
    "forecast_period = 24\n",
    "gap = 0 \n",
    "forecast_gap = 0\n",
    "\n",
    "# Lstm parameters\n",
    "hidden_size = 400\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194e4f3040adb64d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Target location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed10c4c3f4d379e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_aus = pd.read_parquet('../data/australia/aus_production.parquet', engine='pyarrow')\n",
    "data_aus = data_aus[data_aus['Customer'] == system_id]\n",
    "data_aus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "693853afcf60f671"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparams from the data\n",
    "peak_power = data_aus['Generator Capacity'].iloc[0]\n",
    "latitude = data_aus['latitude'].iloc[0]\n",
    "longitude = data_aus['longitude'].iloc[0]\n",
    "\n",
    "# Hyperparams not included in the data\n",
    "tilt = 0\n",
    "azimuth = 0\n",
    "# The optimal angles replaces the tilt and azimuth by \"ideal\" settings\n",
    "optimalangles = True\n",
    "\n",
    "latitude, longitude, peak_power, tilt, azimuth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "879cf97881ff0aa1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Unique name for the data, model and metrics\n",
    "data_name = 'base_' + 'australia' '_' + str(system_id)\n",
    "data_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b98fe610c8948df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the folders to save the data and models\n",
    "data_folder = '../results/AUS/'\n",
    "model_folder = '../models/AUS/' + data_name\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86a576bc67d4a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform the dataframe to one retaining only the power output\n",
    "data_aus = pd.DataFrame(data_aus['Values'])\n",
    "data_aus = data_aus.resample('H').sum()\n",
    "data_aus = data_aus.rename(columns={\"Values\":\"P\"})\n",
    "\n",
    "target_data = data_aus\n",
    "target_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c19895e1ca9836a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Source location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baaa56493597bd4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fetch data from PVGIS\n",
    "data_PVGIS = PvFetcher(latitude,longitude,peak_power, tilt, azimuth, locations=locations_used, start_date=start_date, end_date=end_date,optimal_angles=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30957dec88587bd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the data from the fetcher\n",
    "data = [data_PVGIS.dataset[0]]\n",
    "\n",
    "# Localize the data so that hours align\n",
    "data[0] = data[0].tz_localize('UTC').tz_convert('Australia/Sydney').tz_localize(None)\n",
    "\n",
    "# Remove the hours before and after that do not make up a complete day\n",
    "data[0] = data[0][13:-11]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a9c09603b07cd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Featurisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0927a4cbed4240"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1ac8eb1137604f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53875ac2a81ecd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decide on the features to use in making the model (Note that 'P' should always be included since it's the target variable)\n",
    "dataset = Featurisation(data).base_features(past_features)\n",
    "\n",
    "# Include cyclical features\n",
    "dataset = Featurisation(dataset).cyclic_features(yearly=False)\n",
    "features = dataset[0].columns # update the features\n",
    "source_data = dataset[0].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f205764b73ce294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f3f7a9d43b9fd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da22dbc59d930a02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identical to the source domain\n",
    "target_featurisation = Featurisation([target_data])\n",
    "target_data = target_featurisation.cyclic_features()[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f13f0391f4a6fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Include domain knowledge into the target domain for scaling purposes\n",
    "# We know that the minimum power is always 0\n",
    "domain_min = [0.0]\n",
    "# We are going to assume that the maximum is the peak rated power times some degradation factor. In the paper we assume this degradation is 14%, this is the number also used by PVGIS\n",
    "# cf. https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/getting-started-pvgis/pvgis-user-manual_en#ref-9-hourly-solar-radiation-and-pv-data\n",
    "domain_max = [peak_power*0.86]\n",
    "\n",
    "# For other features we just assume that the minimum and maximum are what we have seen in the source data, this data is freely available, so this is not a stretch\n",
    "other_features = past_features[1:] + future_features\n",
    "for i in range(len(other_features)):\n",
    "    domain_min.append(min(source_data[other_features[i]]))\n",
    "    domain_max.append(max(source_data[other_features[i]]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442eef9d63a6aad9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33755eed0dac848c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f798611c3d20350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the data in the torch.tensor format\n",
    "src_tensors = Tensors(source_data, 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap)\n",
    "\n",
    "# Split the data into train and test sets with separate tensors for features (X) and the target (y)\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = src_tensors.create_tensor()\n",
    "X_train_src.shape, X_test_src.shape, y_train_src.shape, y_test_src.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e368d2ed7f223ac4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5659b927ee23db17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the target dataset we require a separate \"evaluation set\" of a full year, apart from the train and test set. This makes the tensorisation of the data a bit more complex than what we did for the source domain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f90c883f4676c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take apart the train and test data\n",
    "target_excl_eval = target_data[:-365*24]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79aaf7cd590d4383"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the months we have available for training. We need this info to make separate cases for each unique case of having \"X months\" of data in the target domain\n",
    "training_months = list(target_excl_eval.index.month.unique())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa969d15791db38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the timestamps of the training start points for each case of having \"X months\" of data\n",
    "train_starts = []\n",
    "for i in range(len(training_months)):\n",
    "    train_start = target_excl_eval[(target_excl_eval.index.month ==training_months[i])].index[0]\n",
    "    train_starts.append(train_start)\n",
    "    \n",
    "train_starts = list(reversed(train_starts))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a38ed6bf8227881"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the target data in lists holding all the tensors for each of the \"X months\" cases. This time with a train and test set, as well as a separate evaluation set. \n",
    "X_train_target_list = []\n",
    "X_test_target_list = []\n",
    "X_eval_target_list = []\n",
    "y_train_target_list = []\n",
    "y_test_target_list = []\n",
    "y_eval_target_list = []\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    tgt_tensors = Tensors(target_data.loc[train_starts[i]:], 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap, evaluation_length=24*365, domain_min=domain_min, domain_max=domain_max)\n",
    "    X_train_tgt, X_test_tgt, X_eval_tgt, y_train_tgt, y_test_tgt, y_eval_tgt = tgt_tensors.create_tensor()\n",
    "    X_train_target_list.append(X_train_tgt)\n",
    "    X_test_target_list.append(X_test_tgt)\n",
    "    X_eval_target_list.append(X_eval_tgt)\n",
    "    y_train_target_list.append(y_train_tgt)\n",
    "    y_test_target_list.append(y_test_tgt)\n",
    "    y_eval_target_list.append(y_eval_tgt) \n",
    "    print(X_train_tgt.shape, X_test_tgt.shape, X_eval_tgt.shape, y_train_tgt.shape, y_test_tgt.shape, y_eval_tgt.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cdbbb4f080be078"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Source model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3786957ed4b4b06b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "my_lstm = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "my_lstm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5bf308c1fc0b7bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "training = Training(my_lstm, X_train_src, y_train_src, X_test_src, y_test_src, epochs,batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model and return the trained parameters and the best iteration\n",
    "state_dict_list, best_epoch = training.fit()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17feb56dd0a55ab0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the state dictionary of the best performing model\n",
    "my_lstm.load_state_dict(state_dict_list[best_epoch])\n",
    "\n",
    "# Save the model state dictionary for later use \n",
    "save_model(my_lstm, 'AUS/' + data_name + '/model_' + data_name + '_transfer_0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78309de0faa97b09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Forecast with the model\n",
    "forecasts = my_lstm(X_test_src.to(device))\n",
    "\n",
    "# Evaluate the model performance\n",
    "source_eval = Evaluation(y_test_src.detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc2960f198f7417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Target model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed943c74f35620"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "# Create empty models for each of the periods\n",
    "target_lstm_list = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    target_lstm_list.append(LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device))\n",
    "\n",
    "# The \"0 months\" case is basically random initialization of the weights, so we can already save this model as the target_0 model    \n",
    "torch.save(target_lstm_list[0].state_dict(), '../models/AUS/' + data_name + '/model_' + data_name + '_target_0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172a0306b3cfa853"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep track of the best performing iteration\n",
    "target_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Initialize the trainer\n",
    "    training = Training(target_lstm_list[i+1], X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    target_lstm_list[i+1].load_state_dict(state_dict_list[best_epoch])\n",
    "    target_best_epochs.append(best_epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a707dd96e1a14b7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Maintain lists with all three evaluation metrics used in the paper\n",
    "target_RMSEs = []\n",
    "target_MBEs = []\n",
    "target_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "forecasts = target_lstm_list[0](X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "target_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = target_lstm_list[i+1](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Append the evaluation metrics\n",
    "    target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    target_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0660e76f1cde60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Transfer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1aa75c7d725d84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We freeze the weights and biases of the first layer\n",
    "freezing = []\n",
    "\n",
    "for name, _ in my_lstm.lstm.named_parameters():\n",
    "    freezing.append(name)\n",
    "    \n",
    "freezing = freezing[:4]\n",
    "freezing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a1ee9bd9bff6639"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transfer_models = []\n",
    "transfer_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    transfer_model  = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "    transfer_model.load_state_dict(torch.load('../models/AUS/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "       \n",
    "    for name, param in transfer_model.lstm.named_parameters():\n",
    "        if any(freezing_name in name for freezing_name in freezing):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Initialize the trainer\n",
    "    training = Training(transfer_model, \n",
    "                              X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], \n",
    "                              epochs=epochs, batch_size = batch_size, learning_rate =learning_rate/100)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    transfer_model.load_state_dict(state_dict_list[best_epoch])\n",
    "    \n",
    "    transfer_best_epochs.append(best_epoch)\n",
    "    transfer_models.append(transfer_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "350c3b23d316dd6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transfer_RMSEs = []\n",
    "transfer_MBEs = []\n",
    "transfer_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "\n",
    "transfer_model = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "transfer_model.load_state_dict(torch.load('../models/AUS/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "\n",
    "forecasts = transfer_model(X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = transfer_models[i](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5a423788b3e209"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa98825b51380"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_RMSEs = []\n",
    "baseline_MBEs = []\n",
    "baseline_MAEs = []\n",
    "\n",
    "# Evaluate a clean model, our forecast in this case is basically the first feature in our features tensor, as we predict the next day to be the previous one \n",
    "forecasts = X_eval_target_list[0][:,:,0]\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = X_eval_target_list[i][:,:,0]\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9a67df5080bb4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Final visualisation and export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19632e7bcce954a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(target_RMSEs,label='target')\n",
    "plt.plot(transfer_RMSEs,label='transfer')\n",
    "plt.plot(baseline_RMSEs, label='baseline')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a796e9ba899736a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_names = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    column_names.append(str(i) + 'm')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f738ffd4e8f96d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_metrics = pd.DataFrame([baseline_RMSEs, target_RMSEs, transfer_RMSEs,\n",
    "                            baseline_MBEs, target_MBEs, transfer_MBEs,\n",
    "                            baseline_MAEs, target_MAEs, transfer_MAEs, \n",
    "                            target_best_epochs, transfer_best_epochs],\n",
    "                           columns=column_names, index=['Baseline RMSE', 'Target RMSE', 'Transfer RMSE', \n",
    "                                                        'Baseline MBE', 'Target MBE', 'Transfer MBE', \n",
    "                                                        'Baseline MAE', 'Target MAE', 'Transfer MAE', \n",
    "                                                        'Target epoch', 'Transfer epoch']).transpose()\n",
    "\n",
    "all_metrics['Target epoch'] = all_metrics['Target epoch'].astype(int)\n",
    "all_metrics['Transfer epoch'] = all_metrics['Transfer epoch'].astype(int)\n",
    "all_metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cc8e6ba8e067e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_metrics.to_csv('../results/AUS/' + 'summary_table_' + data_name + '.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e911dfa55c19f92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62ad32304b2e2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
