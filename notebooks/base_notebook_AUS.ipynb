{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# THIS PARAMETER IS USED FOR LOOPING OVER THE NOTEBOOK IN THE \"run_all_customers\" NOTEBOOK\n",
    "system_id = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:32.986230800Z",
     "start_time": "2025-03-12T17:40:32.890483900Z"
    }
   },
   "id": "49eef3da1999505"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.480045900Z",
     "start_time": "2025-03-12T17:40:32.894658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base library imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SolNet imports\n",
    "from src.data.datafetcher import PvFetcher\n",
    "from src.data.featurisation import Featurisation\n",
    "from src.tensors.tensorisation import Tensors\n",
    "from src.models.lstm import LSTM\n",
    "from src.models.training import Training\n",
    "from src.models.training import save_model\n",
    "from src.evaluation.evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Hyperparameters needed for a run:\n",
    "\n",
    "# Data fetching\n",
    "locations_used = 1\n",
    "start_date = 2005\n",
    "end_date = 2010\n",
    "\n",
    "# Forecasting parameters\n",
    "target = 'P'\n",
    "past_features = ['P']\n",
    "future_features = ['hour_sin','hour_cos']\n",
    "lags = 24\n",
    "forecast_period = 24\n",
    "gap = 0 \n",
    "forecast_gap = 0\n",
    "\n",
    "# Lstm parameters\n",
    "hidden_size = 400\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.575831700Z",
     "start_time": "2025-03-12T17:40:37.569418900Z"
    }
   },
   "id": "194e4f3040adb64d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Target location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed10c4c3f4d379e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                     Customer  Postcode  Generator Capacity  Values  \\\ndate                                                                  \n2011-07-01 00:00:00       1.0      2076                3.78     0.0   \n2011-07-01 00:30:00       1.0      2076                3.78     0.0   \n2011-07-01 01:00:00       1.0      2076                3.78     0.0   \n2011-07-01 01:30:00       1.0      2076                3.78     0.0   \n2011-07-01 02:00:00       1.0      2076                3.78     0.0   \n...                       ...       ...                 ...     ...   \n2013-06-30 21:30:00       1.0      2076                3.78     0.0   \n2013-06-30 22:00:00       1.0      2076                3.78     0.0   \n2013-06-30 22:30:00       1.0      2076                3.78     0.0   \n2013-06-30 23:00:00       1.0      2076                3.78     0.0   \n2013-06-30 23:30:00       1.0      2076                3.78     0.0   \n\n                      latitude  longitude  \ndate                                       \n2011-07-01 00:00:00 -33.696992  151.13078  \n2011-07-01 00:30:00 -33.696992  151.13078  \n2011-07-01 01:00:00 -33.696992  151.13078  \n2011-07-01 01:30:00 -33.696992  151.13078  \n2011-07-01 02:00:00 -33.696992  151.13078  \n...                        ...        ...  \n2013-06-30 21:30:00 -33.696992  151.13078  \n2013-06-30 22:00:00 -33.696992  151.13078  \n2013-06-30 22:30:00 -33.696992  151.13078  \n2013-06-30 23:00:00 -33.696992  151.13078  \n2013-06-30 23:30:00 -33.696992  151.13078  \n\n[35088 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Customer</th>\n      <th>Postcode</th>\n      <th>Generator Capacity</th>\n      <th>Values</th>\n      <th>latitude</th>\n      <th>longitude</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2011-07-01 00:00:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 00:30:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 01:00:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 01:30:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 02:00:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 21:30:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 22:00:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 22:30:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 23:00:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 23:30:00</th>\n      <td>1.0</td>\n      <td>2076</td>\n      <td>3.78</td>\n      <td>0.0</td>\n      <td>-33.696992</td>\n      <td>151.13078</td>\n    </tr>\n  </tbody>\n</table>\n<p>35088 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aus = pd.read_parquet('../data/australia/aus_production.parquet', engine='pyarrow')\n",
    "data_aus = data_aus[data_aus['Customer'] == system_id]\n",
    "data_aus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.883799400Z",
     "start_time": "2025-03-12T17:40:37.573835100Z"
    }
   },
   "id": "693853afcf60f671"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(-33.69699153381296, 151.13077966206853, 3.78, 0, 0)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams from the data\n",
    "peak_power = data_aus['Generator Capacity'].iloc[0]\n",
    "latitude = data_aus['latitude'].iloc[0]\n",
    "longitude = data_aus['longitude'].iloc[0]\n",
    "\n",
    "# Hyperparams not included in the data\n",
    "tilt = 0\n",
    "azimuth = 0\n",
    "# The optimal angles replaces the tilt and azimuth by \"ideal\" settings\n",
    "optimalangles = True\n",
    "\n",
    "latitude, longitude, peak_power, tilt, azimuth"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.884793200Z",
     "start_time": "2025-03-12T17:40:37.859251300Z"
    }
   },
   "id": "879cf97881ff0aa1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'base_australia_1'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique name for the data, model and metrics\n",
    "data_name = 'base_' + 'australia' '_' + str(system_id)\n",
    "data_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.885794900Z",
     "start_time": "2025-03-12T17:40:37.864806100Z"
    }
   },
   "id": "6b98fe610c8948df"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create the folders to save the data and models\n",
    "data_folder = '../results/AUS/'\n",
    "model_folder = '../models/AUS/' + data_name\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.885794900Z",
     "start_time": "2025-03-12T17:40:37.873202400Z"
    }
   },
   "id": "d86a576bc67d4a1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                       P\ndate                    \n2011-07-01 00:00:00  0.0\n2011-07-01 01:00:00  0.0\n2011-07-01 02:00:00  0.0\n2011-07-01 03:00:00  0.0\n2011-07-01 04:00:00  0.0\n...                  ...\n2013-06-30 19:00:00  0.0\n2013-06-30 20:00:00  0.0\n2013-06-30 21:00:00  0.0\n2013-06-30 22:00:00  0.0\n2013-06-30 23:00:00  0.0\n\n[17544 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2011-07-01 00:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 01:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 02:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 03:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011-07-01 04:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 19:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 20:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 21:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 22:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-06-30 23:00:00</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17544 rows Ã— 1 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the dataframe to one retaining only the power output\n",
    "data_aus = pd.DataFrame(data_aus['Values'])\n",
    "data_aus = data_aus.resample('H').sum()\n",
    "data_aus = data_aus.rename(columns={\"Values\":\"P\"})\n",
    "\n",
    "target_data = data_aus\n",
    "target_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:37.939072100Z",
     "start_time": "2025-03-12T17:40:37.881474300Z"
    }
   },
   "id": "c19895e1ca9836a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Source location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baaa56493597bd4a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from base location...\n"
     ]
    }
   ],
   "source": [
    "# Fetch data from PVGIS\n",
    "data_PVGIS = PvFetcher(latitude,longitude,peak_power, tilt, azimuth, locations=locations_used, start_date=start_date, end_date=end_date,optimal_angles=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.062940600Z",
     "start_time": "2025-03-12T17:40:37.904638300Z"
    }
   },
   "id": "30957dec88587bd9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Get the data from the fetcher\n",
    "data = [data_PVGIS.dataset[0]]\n",
    "\n",
    "# Localize the data so that hours align\n",
    "data[0] = data[0].tz_localize('UTC').tz_convert('Australia/Sydney').tz_localize(None)\n",
    "\n",
    "# Remove the hours before and after that do not make up a complete day\n",
    "data[0] = data[0][13:-11]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.119707700Z",
     "start_time": "2025-03-12T17:40:44.064891100Z"
    }
   },
   "id": "a6a9c09603b07cd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Featurisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0927a4cbed4240"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1ac8eb1137604f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53875ac2a81ecd0"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Decide on the features to use in making the model (Note that 'P' should always be included since it's the target variable)\n",
    "dataset = Featurisation(data).base_features(past_features)\n",
    "\n",
    "# Include cyclical features\n",
    "dataset = Featurisation(dataset).cyclic_features(yearly=False)\n",
    "features = dataset[0].columns # update the features\n",
    "source_data = dataset[0].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.132429800Z",
     "start_time": "2025-03-12T17:40:44.115657100Z"
    }
   },
   "id": "2f205764b73ce294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f3f7a9d43b9fd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da22dbc59d930a02"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Identical to the source domain\n",
    "target_featurisation = Featurisation([target_data])\n",
    "target_data = target_featurisation.cyclic_features()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.133429300Z",
     "start_time": "2025-03-12T17:40:44.126701500Z"
    }
   },
   "id": "15f13f0391f4a6fb"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Include domain knowledge into the target domain for scaling purposes\n",
    "# We know that the minimum power is always 0\n",
    "domain_min = [0.0]\n",
    "# We are going to assume that the maximum is the peak rated power times some degradation factor. In the paper we assume this degradation is 14%, this is the number also used by PVGIS\n",
    "# cf. https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/getting-started-pvgis/pvgis-user-manual_en#ref-9-hourly-solar-radiation-and-pv-data\n",
    "domain_max = [peak_power*0.86]\n",
    "\n",
    "# For other features we just assume that the minimum and maximum are what we have seen in the source data, this data is freely available, so this is not a stretch\n",
    "other_features = past_features[1:] + future_features\n",
    "for i in range(len(other_features)):\n",
    "    domain_min.append(min(source_data[other_features[i]]))\n",
    "    domain_max.append(max(source_data[other_features[i]]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.166967900Z",
     "start_time": "2025-03-12T17:40:44.136430300Z"
    }
   },
   "id": "442eef9d63a6aad9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33755eed0dac848c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f798611c3d20350"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1751, 24, 3]),\n torch.Size([438, 24, 3]),\n torch.Size([1751, 24]),\n torch.Size([438, 24]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data in the torch.tensor format\n",
    "src_tensors = Tensors(source_data, 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap)\n",
    "\n",
    "# Split the data into train and test sets with separate tensors for features (X) and the target (y)\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = src_tensors.create_tensor()\n",
    "X_train_src.shape, X_test_src.shape, y_train_src.shape, y_test_src.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.199048100Z",
     "start_time": "2025-03-12T17:40:44.154575900Z"
    }
   },
   "id": "e368d2ed7f223ac4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5659b927ee23db17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the target dataset we require a separate \"evaluation set\" of a full year, apart from the train and test set. This makes the tensorisation of the data a bit more complex than what we did for the source domain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f90c883f4676c2"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Take apart the train and test data\n",
    "target_excl_eval = target_data[:-365*24]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.199048100Z",
     "start_time": "2025-03-12T17:40:44.189374300Z"
    }
   },
   "id": "79aaf7cd590d4383"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Get the months we have available for training. We need this info to make separate cases for each unique case of having \"X months\" of data in the target domain\n",
    "training_months = list(target_excl_eval.index.month.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.229288400Z",
     "start_time": "2025-03-12T17:40:44.191392100Z"
    }
   },
   "id": "9aa969d15791db38"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# the timestamps of the training start points for each case of having \"X months\" of data\n",
    "train_starts = []\n",
    "for i in range(len(training_months)):\n",
    "    train_start = target_excl_eval[(target_excl_eval.index.month ==training_months[i])].index[0]\n",
    "    train_starts.append(train_start)\n",
    "    \n",
    "train_starts = list(reversed(train_starts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.246017700Z",
     "start_time": "2025-03-12T17:40:44.200045700Z"
    }
   },
   "id": "9a38ed6bf8227881"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 24, 3]) torch.Size([6, 24, 3]) torch.Size([364, 24, 3]) torch.Size([23, 24]) torch.Size([6, 24]) torch.Size([364, 24])\n",
      "torch.Size([48, 24, 3]) torch.Size([12, 24, 3]) torch.Size([364, 24, 3]) torch.Size([48, 24]) torch.Size([12, 24]) torch.Size([364, 24])\n",
      "torch.Size([72, 24, 3]) torch.Size([18, 24, 3]) torch.Size([364, 24, 3]) torch.Size([72, 24]) torch.Size([18, 24]) torch.Size([364, 24])\n",
      "torch.Size([97, 24, 3]) torch.Size([24, 24, 3]) torch.Size([364, 24, 3]) torch.Size([97, 24]) torch.Size([24, 24]) torch.Size([364, 24])\n",
      "torch.Size([120, 24, 3]) torch.Size([30, 24, 3]) torch.Size([364, 24, 3]) torch.Size([120, 24]) torch.Size([30, 24]) torch.Size([364, 24])\n",
      "torch.Size([145, 24, 3]) torch.Size([36, 24, 3]) torch.Size([364, 24, 3]) torch.Size([145, 24]) torch.Size([36, 24]) torch.Size([364, 24])\n",
      "torch.Size([170, 24, 3]) torch.Size([42, 24, 3]) torch.Size([364, 24, 3]) torch.Size([170, 24]) torch.Size([42, 24]) torch.Size([364, 24])\n",
      "torch.Size([194, 24, 3]) torch.Size([48, 24, 3]) torch.Size([364, 24, 3]) torch.Size([194, 24]) torch.Size([48, 24]) torch.Size([364, 24])\n",
      "torch.Size([218, 24, 3]) torch.Size([55, 24, 3]) torch.Size([364, 24, 3]) torch.Size([218, 24]) torch.Size([55, 24]) torch.Size([364, 24])\n",
      "torch.Size([242, 24, 3]) torch.Size([61, 24, 3]) torch.Size([364, 24, 3]) torch.Size([242, 24]) torch.Size([61, 24]) torch.Size([364, 24])\n",
      "torch.Size([267, 24, 3]) torch.Size([67, 24, 3]) torch.Size([364, 24, 3]) torch.Size([267, 24]) torch.Size([67, 24]) torch.Size([364, 24])\n",
      "torch.Size([292, 24, 3]) torch.Size([73, 24, 3]) torch.Size([364, 24, 3]) torch.Size([292, 24]) torch.Size([73, 24]) torch.Size([364, 24])\n"
     ]
    }
   ],
   "source": [
    "# Get the target data in lists holding all the tensors for each of the \"X months\" cases. This time with a train and test set, as well as a separate evaluation set. \n",
    "X_train_target_list = []\n",
    "X_test_target_list = []\n",
    "X_eval_target_list = []\n",
    "y_train_target_list = []\n",
    "y_test_target_list = []\n",
    "y_eval_target_list = []\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    tgt_tensors = Tensors(target_data.loc[train_starts[i]:], 'P', past_features , future_features, lags, forecast_period, gap=gap, forecast_gap=forecast_gap, evaluation_length=24*365, domain_min=domain_min, domain_max=domain_max)\n",
    "    X_train_tgt, X_test_tgt, X_eval_tgt, y_train_tgt, y_test_tgt, y_eval_tgt = tgt_tensors.create_tensor()\n",
    "    X_train_target_list.append(X_train_tgt)\n",
    "    X_test_target_list.append(X_test_tgt)\n",
    "    X_eval_target_list.append(X_eval_tgt)\n",
    "    y_train_target_list.append(y_train_tgt)\n",
    "    y_test_target_list.append(y_test_tgt)\n",
    "    y_eval_target_list.append(y_eval_tgt) \n",
    "    print(X_train_tgt.shape, X_test_tgt.shape, X_eval_tgt.shape, y_train_tgt.shape, y_test_tgt.shape, y_eval_tgt.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.248076900Z",
     "start_time": "2025-03-12T17:40:44.210007500Z"
    }
   },
   "id": "3cdbbb4f080be078"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Source model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3786957ed4b4b06b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "LSTM(\n  (lstm): LSTM(3, 400, num_layers=3, batch_first=True, dropout=0.5)\n  (linear): Linear(in_features=400, out_features=24, bias=True)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "my_lstm = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "my_lstm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:40:44.527358800Z",
     "start_time": "2025-03-12T17:40:44.235053600Z"
    }
   },
   "id": "b5bf308c1fc0b7bb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Average train loss: 0.0342 | Average test loss: 0.0148\n",
      "Step 5: Average train loss: 0.0086 | Average test loss: 0.0108\n",
      "Step 10: Average train loss: 0.0080 | Average test loss: 0.0105\n",
      "Step 15: Average train loss: 0.0079 | Average test loss: 0.0103\n",
      "Step 20: Average train loss: 0.0077 | Average test loss: 0.0102\n",
      "Step 25: Average train loss: 0.0076 | Average test loss: 0.0101\n",
      "Step 30: Average train loss: 0.0075 | Average test loss: 0.0100\n",
      "Step 35: Average train loss: 0.0075 | Average test loss: 0.0099\n",
      "Step 40: Average train loss: 0.0075 | Average test loss: 0.0098\n",
      "Step 45: Average train loss: 0.0074 | Average test loss: 0.0098\n",
      "Step 50: Average train loss: 0.0073 | Average test loss: 0.0097\n",
      "Step 55: Average train loss: 0.0072 | Average test loss: 0.0097\n",
      "Step 60: Average train loss: 0.0072 | Average test loss: 0.0095\n",
      "Step 65: Average train loss: 0.0071 | Average test loss: 0.0095\n",
      "Step 70: Average train loss: 0.0071 | Average test loss: 0.0094\n",
      "Step 75: Average train loss: 0.0071 | Average test loss: 0.0093\n",
      "Step 80: Average train loss: 0.0070 | Average test loss: 0.0093\n",
      "Step 85: Average train loss: 0.0070 | Average test loss: 0.0092\n",
      "Step 90: Average train loss: 0.0070 | Average test loss: 0.0092\n",
      "Step 95: Average train loss: 0.0070 | Average test loss: 0.0092\n",
      "Step 100: Average train loss: 0.0069 | Average test loss: 0.0092\n",
      "Step 105: Average train loss: 0.0069 | Average test loss: 0.0091\n",
      "Step 110: Average train loss: 0.0069 | Average test loss: 0.0091\n",
      "Step 115: Average train loss: 0.0069 | Average test loss: 0.0091\n",
      "Step 120: Average train loss: 0.0069 | Average test loss: 0.0091\n",
      "Step 125: Average train loss: 0.0069 | Average test loss: 0.0091\n",
      "Step 130: Average train loss: 0.0068 | Average test loss: 0.0091\n",
      "Step 135: Average train loss: 0.0068 | Average test loss: 0.0091\n",
      "Step 140: Average train loss: 0.0068 | Average test loss: 0.0091\n",
      "Step 145: Average train loss: 0.0067 | Average test loss: 0.0090\n",
      "Step 150: Average train loss: 0.0068 | Average test loss: 0.0090\n",
      "Step 155: Average train loss: 0.0067 | Average test loss: 0.0089\n",
      "Step 160: Average train loss: 0.0067 | Average test loss: 0.0089\n",
      "Step 165: Average train loss: 0.0067 | Average test loss: 0.0089\n",
      "Step 170: Average train loss: 0.0066 | Average test loss: 0.0089\n",
      "Step 175: Average train loss: 0.0065 | Average test loss: 0.0088\n",
      "Step 180: Average train loss: 0.0065 | Average test loss: 0.0089\n",
      "Step 185: Average train loss: 0.0066 | Average test loss: 0.0088\n",
      "Step 190: Average train loss: 0.0065 | Average test loss: 0.0089\n",
      "Step 195: Average train loss: 0.0064 | Average test loss: 0.0088\n",
      "Best Epoch: 187\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "training = Training(my_lstm, X_train_src, y_train_src, X_test_src, y_test_src, epochs,batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model and return the trained parameters and the best iteration\n",
    "state_dict_list, best_epoch = training.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:41:53.526495300Z",
     "start_time": "2025-03-12T17:40:44.519939200Z"
    }
   },
   "id": "17feb56dd0a55ab0"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Load the state dictionary of the best performing model\n",
    "my_lstm.load_state_dict(state_dict_list[best_epoch])\n",
    "\n",
    "# Save the model state dictionary for later use \n",
    "save_model(my_lstm, 'AUS/' + data_name + '/model_' + data_name + '_transfer_0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:41:53.569420600Z",
     "start_time": "2025-03-12T17:41:53.519747500Z"
    }
   },
   "id": "78309de0faa97b09"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Forecast with the model\n",
    "forecasts = my_lstm(X_test_src.to(device))\n",
    "\n",
    "# Evaluate the model performance\n",
    "source_eval = Evaluation(y_test_src.detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:41:53.583316300Z",
     "start_time": "2025-03-12T17:41:53.553997900Z"
    }
   },
   "id": "cbc2960f198f7417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Target model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed943c74f35620"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Set the parameters for the lstm\n",
    "input_size = len(past_features + future_features)\n",
    "\n",
    "# Create empty models for each of the periods\n",
    "target_lstm_list = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    target_lstm_list.append(LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device))\n",
    "\n",
    "# The \"0 months\" case is basically random initialization of the weights, so we can already save this model as the target_0 model    \n",
    "torch.save(target_lstm_list[0].state_dict(), '../models/AUS/' + data_name + '/model_' + data_name + '_target_0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:41:53.762722800Z",
     "start_time": "2025-03-12T17:41:53.575859300Z"
    }
   },
   "id": "172a0306b3cfa853"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Average train loss: 0.0409 | Average test loss: 0.0433\n",
      "Step 5: Average train loss: 0.0356 | Average test loss: 0.0374\n",
      "Step 10: Average train loss: 0.0301 | Average test loss: 0.0308\n",
      "Step 15: Average train loss: 0.0234 | Average test loss: 0.0222\n",
      "Step 20: Average train loss: 0.0145 | Average test loss: 0.0107\n",
      "Step 25: Average train loss: 0.0130 | Average test loss: 0.0075\n",
      "Step 30: Average train loss: 0.0102 | Average test loss: 0.0068\n",
      "Step 35: Average train loss: 0.0108 | Average test loss: 0.0084\n",
      "Step 40: Average train loss: 0.0106 | Average test loss: 0.0075\n",
      "Step 45: Average train loss: 0.0106 | Average test loss: 0.0064\n",
      "Step 50: Average train loss: 0.0107 | Average test loss: 0.0063\n",
      "Step 55: Average train loss: 0.0104 | Average test loss: 0.0067\n",
      "Step 60: Average train loss: 0.0101 | Average test loss: 0.0068\n",
      "Step 65: Average train loss: 0.0104 | Average test loss: 0.0066\n",
      "Step 70: Average train loss: 0.0104 | Average test loss: 0.0064\n",
      "Step 75: Average train loss: 0.0105 | Average test loss: 0.0065\n",
      "Step 80: Average train loss: 0.0102 | Average test loss: 0.0067\n",
      "Step 85: Average train loss: 0.0100 | Average test loss: 0.0066\n",
      "Step 90: Average train loss: 0.0102 | Average test loss: 0.0065\n",
      "Step 95: Average train loss: 0.0101 | Average test loss: 0.0065\n",
      "Step 100: Average train loss: 0.0100 | Average test loss: 0.0065\n",
      "Step 105: Average train loss: 0.0103 | Average test loss: 0.0065\n",
      "Step 110: Average train loss: 0.0103 | Average test loss: 0.0065\n",
      "Step 115: Average train loss: 0.0103 | Average test loss: 0.0065\n",
      "Step 120: Average train loss: 0.0098 | Average test loss: 0.0065\n",
      "Step 125: Average train loss: 0.0102 | Average test loss: 0.0065\n",
      "Step 130: Average train loss: 0.0100 | Average test loss: 0.0065\n",
      "Step 135: Average train loss: 0.0100 | Average test loss: 0.0065\n",
      "Step 140: Average train loss: 0.0100 | Average test loss: 0.0065\n",
      "Step 145: Average train loss: 0.0103 | Average test loss: 0.0065\n",
      "Step 150: Average train loss: 0.0099 | Average test loss: 0.0065\n",
      "Step 155: Average train loss: 0.0100 | Average test loss: 0.0064\n",
      "Step 160: Average train loss: 0.0101 | Average test loss: 0.0065\n",
      "Step 165: Average train loss: 0.0103 | Average test loss: 0.0065\n",
      "Step 170: Average train loss: 0.0100 | Average test loss: 0.0064\n",
      "Step 175: Average train loss: 0.0101 | Average test loss: 0.0064\n",
      "Step 180: Average train loss: 0.0099 | Average test loss: 0.0064\n",
      "Step 185: Average train loss: 0.0099 | Average test loss: 0.0064\n",
      "Step 190: Average train loss: 0.0102 | Average test loss: 0.0064\n",
      "Step 195: Average train loss: 0.0099 | Average test loss: 0.0064\n",
      "Best Epoch: 28\n",
      "Step 0: Average train loss: 0.0469 | Average test loss: 0.0423\n",
      "Step 5: Average train loss: 0.0361 | Average test loss: 0.0313\n",
      "Step 10: Average train loss: 0.0198 | Average test loss: 0.0130\n",
      "Step 15: Average train loss: 0.0112 | Average test loss: 0.0072\n",
      "Step 20: Average train loss: 0.0112 | Average test loss: 0.0077\n",
      "Step 25: Average train loss: 0.0103 | Average test loss: 0.0073\n",
      "Step 30: Average train loss: 0.0101 | Average test loss: 0.0072\n",
      "Step 35: Average train loss: 0.0105 | Average test loss: 0.0071\n",
      "Step 40: Average train loss: 0.0104 | Average test loss: 0.0071\n",
      "Step 45: Average train loss: 0.0104 | Average test loss: 0.0071\n",
      "Step 50: Average train loss: 0.0104 | Average test loss: 0.0071\n",
      "Step 55: Average train loss: 0.0103 | Average test loss: 0.0071\n",
      "Step 60: Average train loss: 0.0103 | Average test loss: 0.0071\n",
      "Step 65: Average train loss: 0.0103 | Average test loss: 0.0071\n",
      "Step 70: Average train loss: 0.0102 | Average test loss: 0.0071\n",
      "Step 75: Average train loss: 0.0105 | Average test loss: 0.0071\n",
      "Step 80: Average train loss: 0.0103 | Average test loss: 0.0071\n",
      "Step 85: Average train loss: 0.0102 | Average test loss: 0.0071\n",
      "Step 90: Average train loss: 0.0099 | Average test loss: 0.0070\n",
      "Step 95: Average train loss: 0.0100 | Average test loss: 0.0070\n",
      "Step 100: Average train loss: 0.0098 | Average test loss: 0.0070\n",
      "Step 105: Average train loss: 0.0101 | Average test loss: 0.0070\n",
      "Step 110: Average train loss: 0.0100 | Average test loss: 0.0070\n",
      "Step 115: Average train loss: 0.0097 | Average test loss: 0.0070\n",
      "Step 120: Average train loss: 0.0099 | Average test loss: 0.0070\n",
      "Step 125: Average train loss: 0.0095 | Average test loss: 0.0070\n",
      "Step 130: Average train loss: 0.0096 | Average test loss: 0.0070\n",
      "Step 135: Average train loss: 0.0095 | Average test loss: 0.0071\n",
      "Step 140: Average train loss: 0.0092 | Average test loss: 0.0071\n",
      "Step 145: Average train loss: 0.0090 | Average test loss: 0.0071\n",
      "Step 150: Average train loss: 0.0092 | Average test loss: 0.0072\n",
      "Step 155: Average train loss: 0.0089 | Average test loss: 0.0073\n",
      "Step 160: Average train loss: 0.0094 | Average test loss: 0.0073\n",
      "Step 165: Average train loss: 0.0090 | Average test loss: 0.0073\n",
      "Step 170: Average train loss: 0.0092 | Average test loss: 0.0073\n",
      "Step 175: Average train loss: 0.0094 | Average test loss: 0.0073\n",
      "Step 180: Average train loss: 0.0090 | Average test loss: 0.0073\n",
      "Step 185: Average train loss: 0.0090 | Average test loss: 0.0073\n",
      "Step 190: Average train loss: 0.0092 | Average test loss: 0.0073\n",
      "Step 195: Average train loss: 0.0090 | Average test loss: 0.0073\n",
      "Best Epoch: 123\n",
      "Step 0: Average train loss: 0.0628 | Average test loss: 0.0480\n",
      "Step 5: Average train loss: 0.0388 | Average test loss: 0.0238\n",
      "Step 10: Average train loss: 0.0116 | Average test loss: 0.0084\n",
      "Step 15: Average train loss: 0.0124 | Average test loss: 0.0094\n",
      "Step 20: Average train loss: 0.0121 | Average test loss: 0.0084\n",
      "Step 25: Average train loss: 0.0119 | Average test loss: 0.0087\n",
      "Step 30: Average train loss: 0.0119 | Average test loss: 0.0086\n",
      "Step 35: Average train loss: 0.0116 | Average test loss: 0.0085\n",
      "Step 40: Average train loss: 0.0117 | Average test loss: 0.0087\n",
      "Step 45: Average train loss: 0.0116 | Average test loss: 0.0086\n",
      "Step 50: Average train loss: 0.0116 | Average test loss: 0.0086\n",
      "Step 55: Average train loss: 0.0115 | Average test loss: 0.0086\n",
      "Step 60: Average train loss: 0.0114 | Average test loss: 0.0086\n",
      "Step 65: Average train loss: 0.0111 | Average test loss: 0.0086\n",
      "Step 70: Average train loss: 0.0111 | Average test loss: 0.0086\n",
      "Step 75: Average train loss: 0.0109 | Average test loss: 0.0086\n",
      "Step 80: Average train loss: 0.0107 | Average test loss: 0.0087\n",
      "Step 85: Average train loss: 0.0104 | Average test loss: 0.0086\n",
      "Step 90: Average train loss: 0.0102 | Average test loss: 0.0088\n",
      "Step 95: Average train loss: 0.0100 | Average test loss: 0.0087\n",
      "Step 100: Average train loss: 0.0101 | Average test loss: 0.0087\n",
      "Step 105: Average train loss: 0.0100 | Average test loss: 0.0088\n",
      "Step 110: Average train loss: 0.0099 | Average test loss: 0.0088\n",
      "Step 115: Average train loss: 0.0101 | Average test loss: 0.0087\n",
      "Step 120: Average train loss: 0.0099 | Average test loss: 0.0088\n",
      "Step 125: Average train loss: 0.0102 | Average test loss: 0.0089\n",
      "Step 130: Average train loss: 0.0099 | Average test loss: 0.0088\n",
      "Step 135: Average train loss: 0.0098 | Average test loss: 0.0088\n",
      "Step 140: Average train loss: 0.0101 | Average test loss: 0.0088\n",
      "Step 145: Average train loss: 0.0100 | Average test loss: 0.0088\n",
      "Step 150: Average train loss: 0.0099 | Average test loss: 0.0089\n",
      "Step 155: Average train loss: 0.0100 | Average test loss: 0.0089\n",
      "Step 160: Average train loss: 0.0104 | Average test loss: 0.0089\n",
      "Step 165: Average train loss: 0.0100 | Average test loss: 0.0089\n",
      "Step 170: Average train loss: 0.0099 | Average test loss: 0.0089\n",
      "Step 175: Average train loss: 0.0101 | Average test loss: 0.0090\n",
      "Step 180: Average train loss: 0.0102 | Average test loss: 0.0089\n",
      "Step 185: Average train loss: 0.0100 | Average test loss: 0.0089\n",
      "Step 190: Average train loss: 0.0099 | Average test loss: 0.0089\n",
      "Step 195: Average train loss: 0.0099 | Average test loss: 0.0090\n",
      "Best Epoch: 19\n",
      "Step 0: Average train loss: 0.0637 | Average test loss: 0.0408\n",
      "Step 5: Average train loss: 0.0312 | Average test loss: 0.0121\n",
      "Step 10: Average train loss: 0.0167 | Average test loss: 0.0096\n",
      "Step 15: Average train loss: 0.0150 | Average test loss: 0.0104\n",
      "Step 20: Average train loss: 0.0148 | Average test loss: 0.0108\n",
      "Step 25: Average train loss: 0.0147 | Average test loss: 0.0105\n",
      "Step 30: Average train loss: 0.0149 | Average test loss: 0.0103\n",
      "Step 35: Average train loss: 0.0147 | Average test loss: 0.0103\n",
      "Step 40: Average train loss: 0.0150 | Average test loss: 0.0101\n",
      "Step 45: Average train loss: 0.0149 | Average test loss: 0.0104\n",
      "Step 50: Average train loss: 0.0149 | Average test loss: 0.0102\n",
      "Step 55: Average train loss: 0.0137 | Average test loss: 0.0098\n",
      "Step 60: Average train loss: 0.0135 | Average test loss: 0.0098\n",
      "Step 65: Average train loss: 0.0131 | Average test loss: 0.0095\n",
      "Step 70: Average train loss: 0.0128 | Average test loss: 0.0096\n",
      "Step 75: Average train loss: 0.0129 | Average test loss: 0.0096\n",
      "Step 80: Average train loss: 0.0129 | Average test loss: 0.0095\n",
      "Step 85: Average train loss: 0.0127 | Average test loss: 0.0095\n",
      "Step 90: Average train loss: 0.0126 | Average test loss: 0.0094\n",
      "Step 95: Average train loss: 0.0127 | Average test loss: 0.0094\n",
      "Step 100: Average train loss: 0.0126 | Average test loss: 0.0094\n",
      "Step 105: Average train loss: 0.0125 | Average test loss: 0.0095\n",
      "Step 110: Average train loss: 0.0122 | Average test loss: 0.0094\n",
      "Step 115: Average train loss: 0.0125 | Average test loss: 0.0095\n",
      "Step 120: Average train loss: 0.0120 | Average test loss: 0.0093\n",
      "Step 125: Average train loss: 0.0120 | Average test loss: 0.0095\n",
      "Step 130: Average train loss: 0.0118 | Average test loss: 0.0095\n",
      "Step 135: Average train loss: 0.0118 | Average test loss: 0.0095\n",
      "Step 140: Average train loss: 0.0119 | Average test loss: 0.0094\n",
      "Step 145: Average train loss: 0.0122 | Average test loss: 0.0094\n",
      "Step 150: Average train loss: 0.0118 | Average test loss: 0.0095\n",
      "Step 155: Average train loss: 0.0118 | Average test loss: 0.0096\n",
      "Step 160: Average train loss: 0.0118 | Average test loss: 0.0095\n",
      "Step 165: Average train loss: 0.0114 | Average test loss: 0.0096\n",
      "Step 170: Average train loss: 0.0115 | Average test loss: 0.0095\n",
      "Step 175: Average train loss: 0.0119 | Average test loss: 0.0096\n",
      "Step 180: Average train loss: 0.0116 | Average test loss: 0.0097\n",
      "Step 185: Average train loss: 0.0115 | Average test loss: 0.0098\n",
      "Step 190: Average train loss: 0.0114 | Average test loss: 0.0101\n",
      "Step 195: Average train loss: 0.0107 | Average test loss: 0.0098\n",
      "Best Epoch: 137\n",
      "Step 0: Average train loss: 0.0963 | Average test loss: 0.0414\n",
      "Step 5: Average train loss: 0.0459 | Average test loss: 0.0118\n",
      "Step 10: Average train loss: 0.0196 | Average test loss: 0.0183\n",
      "Step 15: Average train loss: 0.0176 | Average test loss: 0.0188\n",
      "Step 20: Average train loss: 0.0174 | Average test loss: 0.0191\n",
      "Step 25: Average train loss: 0.0175 | Average test loss: 0.0193\n",
      "Step 30: Average train loss: 0.0171 | Average test loss: 0.0192\n",
      "Step 35: Average train loss: 0.0170 | Average test loss: 0.0189\n",
      "Step 40: Average train loss: 0.0170 | Average test loss: 0.0186\n",
      "Step 45: Average train loss: 0.0168 | Average test loss: 0.0185\n",
      "Step 50: Average train loss: 0.0169 | Average test loss: 0.0180\n",
      "Step 55: Average train loss: 0.0166 | Average test loss: 0.0168\n",
      "Step 60: Average train loss: 0.0164 | Average test loss: 0.0162\n",
      "Step 65: Average train loss: 0.0163 | Average test loss: 0.0152\n",
      "Step 70: Average train loss: 0.0160 | Average test loss: 0.0141\n",
      "Step 75: Average train loss: 0.0161 | Average test loss: 0.0136\n",
      "Step 80: Average train loss: 0.0161 | Average test loss: 0.0136\n",
      "Step 85: Average train loss: 0.0158 | Average test loss: 0.0138\n",
      "Step 90: Average train loss: 0.0160 | Average test loss: 0.0139\n",
      "Step 95: Average train loss: 0.0158 | Average test loss: 0.0134\n",
      "Step 100: Average train loss: 0.0158 | Average test loss: 0.0133\n",
      "Step 105: Average train loss: 0.0159 | Average test loss: 0.0132\n",
      "Step 110: Average train loss: 0.0156 | Average test loss: 0.0133\n",
      "Step 115: Average train loss: 0.0159 | Average test loss: 0.0135\n",
      "Step 120: Average train loss: 0.0158 | Average test loss: 0.0134\n",
      "Step 125: Average train loss: 0.0159 | Average test loss: 0.0131\n",
      "Step 130: Average train loss: 0.0157 | Average test loss: 0.0133\n",
      "Step 135: Average train loss: 0.0157 | Average test loss: 0.0130\n",
      "Step 140: Average train loss: 0.0156 | Average test loss: 0.0131\n",
      "Step 145: Average train loss: 0.0155 | Average test loss: 0.0131\n",
      "Step 150: Average train loss: 0.0156 | Average test loss: 0.0130\n",
      "Step 155: Average train loss: 0.0155 | Average test loss: 0.0132\n",
      "Step 160: Average train loss: 0.0155 | Average test loss: 0.0130\n",
      "Step 165: Average train loss: 0.0155 | Average test loss: 0.0129\n",
      "Step 170: Average train loss: 0.0156 | Average test loss: 0.0131\n",
      "Step 175: Average train loss: 0.0152 | Average test loss: 0.0134\n",
      "Step 180: Average train loss: 0.0153 | Average test loss: 0.0129\n",
      "Step 185: Average train loss: 0.0153 | Average test loss: 0.0132\n",
      "Step 190: Average train loss: 0.0154 | Average test loss: 0.0131\n",
      "Step 195: Average train loss: 0.0155 | Average test loss: 0.0126\n",
      "Best Epoch: 5\n",
      "Step 0: Average train loss: 0.0980 | Average test loss: 0.0415\n",
      "Step 5: Average train loss: 0.0317 | Average test loss: 0.0350\n",
      "Step 10: Average train loss: 0.0205 | Average test loss: 0.0179\n",
      "Step 15: Average train loss: 0.0197 | Average test loss: 0.0162\n",
      "Step 20: Average train loss: 0.0193 | Average test loss: 0.0157\n",
      "Step 25: Average train loss: 0.0194 | Average test loss: 0.0155\n",
      "Step 30: Average train loss: 0.0189 | Average test loss: 0.0156\n",
      "Step 35: Average train loss: 0.0190 | Average test loss: 0.0144\n",
      "Step 40: Average train loss: 0.0185 | Average test loss: 0.0131\n",
      "Step 45: Average train loss: 0.0180 | Average test loss: 0.0120\n",
      "Step 50: Average train loss: 0.0179 | Average test loss: 0.0107\n",
      "Step 55: Average train loss: 0.0177 | Average test loss: 0.0104\n",
      "Step 60: Average train loss: 0.0176 | Average test loss: 0.0101\n",
      "Step 65: Average train loss: 0.0176 | Average test loss: 0.0102\n",
      "Step 70: Average train loss: 0.0176 | Average test loss: 0.0099\n",
      "Step 75: Average train loss: 0.0173 | Average test loss: 0.0100\n",
      "Step 80: Average train loss: 0.0175 | Average test loss: 0.0099\n",
      "Step 85: Average train loss: 0.0173 | Average test loss: 0.0098\n",
      "Step 90: Average train loss: 0.0173 | Average test loss: 0.0097\n",
      "Step 95: Average train loss: 0.0172 | Average test loss: 0.0093\n",
      "Step 100: Average train loss: 0.0173 | Average test loss: 0.0092\n",
      "Step 105: Average train loss: 0.0170 | Average test loss: 0.0094\n",
      "Step 110: Average train loss: 0.0172 | Average test loss: 0.0095\n",
      "Step 115: Average train loss: 0.0171 | Average test loss: 0.0095\n",
      "Step 120: Average train loss: 0.0172 | Average test loss: 0.0095\n",
      "Step 125: Average train loss: 0.0170 | Average test loss: 0.0096\n",
      "Step 130: Average train loss: 0.0172 | Average test loss: 0.0093\n",
      "Step 135: Average train loss: 0.0171 | Average test loss: 0.0093\n",
      "Step 140: Average train loss: 0.0173 | Average test loss: 0.0094\n",
      "Step 145: Average train loss: 0.0169 | Average test loss: 0.0095\n",
      "Step 150: Average train loss: 0.0172 | Average test loss: 0.0091\n",
      "Step 155: Average train loss: 0.0168 | Average test loss: 0.0089\n",
      "Step 160: Average train loss: 0.0169 | Average test loss: 0.0089\n",
      "Step 165: Average train loss: 0.0170 | Average test loss: 0.0090\n",
      "Step 170: Average train loss: 0.0168 | Average test loss: 0.0089\n",
      "Step 175: Average train loss: 0.0167 | Average test loss: 0.0087\n",
      "Step 180: Average train loss: 0.0169 | Average test loss: 0.0095\n",
      "Step 185: Average train loss: 0.0164 | Average test loss: 0.0090\n",
      "Step 190: Average train loss: 0.0166 | Average test loss: 0.0088\n",
      "Step 195: Average train loss: 0.0167 | Average test loss: 0.0093\n",
      "Best Epoch: 188\n",
      "Step 0: Average train loss: 0.0919 | Average test loss: 0.0368\n",
      "Step 5: Average train loss: 0.0207 | Average test loss: 0.0147\n",
      "Step 10: Average train loss: 0.0202 | Average test loss: 0.0191\n",
      "Step 15: Average train loss: 0.0198 | Average test loss: 0.0213\n",
      "Step 20: Average train loss: 0.0195 | Average test loss: 0.0206\n",
      "Step 25: Average train loss: 0.0197 | Average test loss: 0.0199\n",
      "Step 30: Average train loss: 0.0192 | Average test loss: 0.0195\n",
      "Step 35: Average train loss: 0.0189 | Average test loss: 0.0180\n",
      "Step 40: Average train loss: 0.0186 | Average test loss: 0.0162\n",
      "Step 45: Average train loss: 0.0184 | Average test loss: 0.0157\n",
      "Step 50: Average train loss: 0.0183 | Average test loss: 0.0155\n",
      "Step 55: Average train loss: 0.0185 | Average test loss: 0.0152\n",
      "Step 60: Average train loss: 0.0182 | Average test loss: 0.0151\n",
      "Step 65: Average train loss: 0.0182 | Average test loss: 0.0150\n",
      "Step 70: Average train loss: 0.0182 | Average test loss: 0.0149\n",
      "Step 75: Average train loss: 0.0181 | Average test loss: 0.0145\n",
      "Step 80: Average train loss: 0.0178 | Average test loss: 0.0145\n",
      "Step 85: Average train loss: 0.0179 | Average test loss: 0.0141\n",
      "Step 90: Average train loss: 0.0179 | Average test loss: 0.0142\n",
      "Step 95: Average train loss: 0.0178 | Average test loss: 0.0139\n",
      "Step 100: Average train loss: 0.0178 | Average test loss: 0.0133\n",
      "Step 105: Average train loss: 0.0177 | Average test loss: 0.0133\n",
      "Step 110: Average train loss: 0.0178 | Average test loss: 0.0135\n",
      "Step 115: Average train loss: 0.0174 | Average test loss: 0.0135\n",
      "Step 120: Average train loss: 0.0174 | Average test loss: 0.0131\n",
      "Step 125: Average train loss: 0.0173 | Average test loss: 0.0133\n",
      "Step 130: Average train loss: 0.0174 | Average test loss: 0.0130\n",
      "Step 135: Average train loss: 0.0173 | Average test loss: 0.0127\n",
      "Step 140: Average train loss: 0.0172 | Average test loss: 0.0127\n",
      "Step 145: Average train loss: 0.0168 | Average test loss: 0.0125\n",
      "Step 150: Average train loss: 0.0169 | Average test loss: 0.0128\n",
      "Step 155: Average train loss: 0.0171 | Average test loss: 0.0130\n",
      "Step 160: Average train loss: 0.0171 | Average test loss: 0.0129\n",
      "Step 165: Average train loss: 0.0169 | Average test loss: 0.0126\n",
      "Step 170: Average train loss: 0.0170 | Average test loss: 0.0128\n",
      "Step 175: Average train loss: 0.0167 | Average test loss: 0.0127\n",
      "Step 180: Average train loss: 0.0168 | Average test loss: 0.0124\n",
      "Step 185: Average train loss: 0.0168 | Average test loss: 0.0121\n",
      "Step 190: Average train loss: 0.0169 | Average test loss: 0.0124\n",
      "Step 195: Average train loss: 0.0168 | Average test loss: 0.0126\n",
      "Best Epoch: 198\n",
      "Step 0: Average train loss: 0.1014 | Average test loss: 0.0414\n",
      "Step 5: Average train loss: 0.0233 | Average test loss: 0.0168\n",
      "Step 10: Average train loss: 0.0211 | Average test loss: 0.0198\n",
      "Step 15: Average train loss: 0.0209 | Average test loss: 0.0198\n",
      "Step 20: Average train loss: 0.0209 | Average test loss: 0.0189\n",
      "Step 25: Average train loss: 0.0203 | Average test loss: 0.0182\n",
      "Step 30: Average train loss: 0.0199 | Average test loss: 0.0159\n",
      "Step 35: Average train loss: 0.0197 | Average test loss: 0.0152\n",
      "Step 40: Average train loss: 0.0198 | Average test loss: 0.0152\n",
      "Step 45: Average train loss: 0.0195 | Average test loss: 0.0150\n",
      "Step 50: Average train loss: 0.0193 | Average test loss: 0.0144\n",
      "Step 55: Average train loss: 0.0194 | Average test loss: 0.0150\n",
      "Step 60: Average train loss: 0.0195 | Average test loss: 0.0141\n",
      "Step 65: Average train loss: 0.0192 | Average test loss: 0.0138\n",
      "Step 70: Average train loss: 0.0192 | Average test loss: 0.0138\n",
      "Step 75: Average train loss: 0.0191 | Average test loss: 0.0141\n",
      "Step 80: Average train loss: 0.0189 | Average test loss: 0.0135\n",
      "Step 85: Average train loss: 0.0190 | Average test loss: 0.0133\n",
      "Step 90: Average train loss: 0.0187 | Average test loss: 0.0127\n",
      "Step 95: Average train loss: 0.0187 | Average test loss: 0.0122\n",
      "Step 100: Average train loss: 0.0188 | Average test loss: 0.0123\n",
      "Step 105: Average train loss: 0.0184 | Average test loss: 0.0124\n",
      "Step 110: Average train loss: 0.0186 | Average test loss: 0.0125\n",
      "Step 115: Average train loss: 0.0187 | Average test loss: 0.0119\n",
      "Step 120: Average train loss: 0.0184 | Average test loss: 0.0115\n",
      "Step 125: Average train loss: 0.0182 | Average test loss: 0.0117\n",
      "Step 130: Average train loss: 0.0183 | Average test loss: 0.0118\n",
      "Step 135: Average train loss: 0.0183 | Average test loss: 0.0122\n",
      "Step 140: Average train loss: 0.0184 | Average test loss: 0.0118\n",
      "Step 145: Average train loss: 0.0182 | Average test loss: 0.0114\n",
      "Step 150: Average train loss: 0.0180 | Average test loss: 0.0118\n",
      "Step 155: Average train loss: 0.0181 | Average test loss: 0.0120\n",
      "Step 160: Average train loss: 0.0184 | Average test loss: 0.0117\n",
      "Step 165: Average train loss: 0.0179 | Average test loss: 0.0112\n",
      "Step 170: Average train loss: 0.0179 | Average test loss: 0.0119\n",
      "Step 175: Average train loss: 0.0181 | Average test loss: 0.0119\n",
      "Step 180: Average train loss: 0.0180 | Average test loss: 0.0115\n",
      "Step 185: Average train loss: 0.0182 | Average test loss: 0.0118\n",
      "Step 190: Average train loss: 0.0177 | Average test loss: 0.0113\n",
      "Step 195: Average train loss: 0.0180 | Average test loss: 0.0114\n",
      "Best Epoch: 184\n",
      "Step 0: Average train loss: 0.1028 | Average test loss: 0.0414\n",
      "Step 5: Average train loss: 0.0255 | Average test loss: 0.0186\n",
      "Step 10: Average train loss: 0.0233 | Average test loss: 0.0223\n",
      "Step 15: Average train loss: 0.0230 | Average test loss: 0.0217\n",
      "Step 20: Average train loss: 0.0227 | Average test loss: 0.0201\n",
      "Step 25: Average train loss: 0.0216 | Average test loss: 0.0180\n",
      "Step 30: Average train loss: 0.0212 | Average test loss: 0.0155\n",
      "Step 35: Average train loss: 0.0209 | Average test loss: 0.0153\n",
      "Step 40: Average train loss: 0.0210 | Average test loss: 0.0152\n",
      "Step 45: Average train loss: 0.0209 | Average test loss: 0.0153\n",
      "Step 50: Average train loss: 0.0208 | Average test loss: 0.0153\n",
      "Step 55: Average train loss: 0.0209 | Average test loss: 0.0148\n",
      "Step 60: Average train loss: 0.0207 | Average test loss: 0.0147\n",
      "Step 65: Average train loss: 0.0208 | Average test loss: 0.0146\n",
      "Step 70: Average train loss: 0.0210 | Average test loss: 0.0146\n",
      "Step 75: Average train loss: 0.0206 | Average test loss: 0.0143\n",
      "Step 80: Average train loss: 0.0203 | Average test loss: 0.0142\n",
      "Step 85: Average train loss: 0.0207 | Average test loss: 0.0142\n",
      "Step 90: Average train loss: 0.0207 | Average test loss: 0.0138\n",
      "Step 95: Average train loss: 0.0201 | Average test loss: 0.0134\n",
      "Step 100: Average train loss: 0.0205 | Average test loss: 0.0137\n",
      "Step 105: Average train loss: 0.0205 | Average test loss: 0.0135\n",
      "Step 110: Average train loss: 0.0202 | Average test loss: 0.0132\n",
      "Step 115: Average train loss: 0.0203 | Average test loss: 0.0132\n",
      "Step 120: Average train loss: 0.0204 | Average test loss: 0.0129\n",
      "Step 125: Average train loss: 0.0203 | Average test loss: 0.0129\n",
      "Step 130: Average train loss: 0.0202 | Average test loss: 0.0130\n",
      "Step 135: Average train loss: 0.0198 | Average test loss: 0.0126\n",
      "Step 140: Average train loss: 0.0201 | Average test loss: 0.0127\n",
      "Step 145: Average train loss: 0.0202 | Average test loss: 0.0126\n",
      "Step 150: Average train loss: 0.0201 | Average test loss: 0.0128\n",
      "Step 155: Average train loss: 0.0198 | Average test loss: 0.0124\n",
      "Step 160: Average train loss: 0.0200 | Average test loss: 0.0124\n",
      "Step 165: Average train loss: 0.0197 | Average test loss: 0.0126\n",
      "Step 170: Average train loss: 0.0200 | Average test loss: 0.0125\n",
      "Step 175: Average train loss: 0.0200 | Average test loss: 0.0127\n",
      "Step 180: Average train loss: 0.0201 | Average test loss: 0.0123\n",
      "Step 185: Average train loss: 0.0198 | Average test loss: 0.0125\n",
      "Step 190: Average train loss: 0.0199 | Average test loss: 0.0125\n",
      "Step 195: Average train loss: 0.0199 | Average test loss: 0.0124\n",
      "Best Epoch: 197\n",
      "Step 0: Average train loss: 0.1009 | Average test loss: 0.0420\n",
      "Step 5: Average train loss: 0.0263 | Average test loss: 0.0244\n",
      "Step 10: Average train loss: 0.0238 | Average test loss: 0.0198\n",
      "Step 15: Average train loss: 0.0235 | Average test loss: 0.0199\n",
      "Step 20: Average train loss: 0.0228 | Average test loss: 0.0185\n",
      "Step 25: Average train loss: 0.0217 | Average test loss: 0.0158\n",
      "Step 30: Average train loss: 0.0217 | Average test loss: 0.0145\n",
      "Step 35: Average train loss: 0.0216 | Average test loss: 0.0144\n",
      "Step 40: Average train loss: 0.0215 | Average test loss: 0.0142\n",
      "Step 45: Average train loss: 0.0216 | Average test loss: 0.0141\n",
      "Step 50: Average train loss: 0.0217 | Average test loss: 0.0142\n",
      "Step 55: Average train loss: 0.0215 | Average test loss: 0.0139\n",
      "Step 60: Average train loss: 0.0213 | Average test loss: 0.0137\n",
      "Step 65: Average train loss: 0.0213 | Average test loss: 0.0137\n",
      "Step 70: Average train loss: 0.0212 | Average test loss: 0.0135\n",
      "Step 75: Average train loss: 0.0216 | Average test loss: 0.0137\n",
      "Step 80: Average train loss: 0.0211 | Average test loss: 0.0136\n",
      "Step 85: Average train loss: 0.0212 | Average test loss: 0.0134\n",
      "Step 90: Average train loss: 0.0214 | Average test loss: 0.0134\n",
      "Step 95: Average train loss: 0.0212 | Average test loss: 0.0130\n",
      "Step 100: Average train loss: 0.0213 | Average test loss: 0.0130\n",
      "Step 105: Average train loss: 0.0211 | Average test loss: 0.0132\n",
      "Step 110: Average train loss: 0.0210 | Average test loss: 0.0128\n",
      "Step 115: Average train loss: 0.0209 | Average test loss: 0.0128\n",
      "Step 120: Average train loss: 0.0211 | Average test loss: 0.0127\n",
      "Step 125: Average train loss: 0.0209 | Average test loss: 0.0122\n",
      "Step 130: Average train loss: 0.0207 | Average test loss: 0.0123\n",
      "Step 135: Average train loss: 0.0208 | Average test loss: 0.0121\n",
      "Step 140: Average train loss: 0.0206 | Average test loss: 0.0122\n",
      "Step 145: Average train loss: 0.0207 | Average test loss: 0.0122\n",
      "Step 150: Average train loss: 0.0205 | Average test loss: 0.0120\n",
      "Step 155: Average train loss: 0.0205 | Average test loss: 0.0122\n",
      "Step 160: Average train loss: 0.0206 | Average test loss: 0.0121\n",
      "Step 165: Average train loss: 0.0204 | Average test loss: 0.0120\n",
      "Step 170: Average train loss: 0.0203 | Average test loss: 0.0118\n",
      "Step 175: Average train loss: 0.0205 | Average test loss: 0.0118\n",
      "Step 180: Average train loss: 0.0202 | Average test loss: 0.0119\n",
      "Step 185: Average train loss: 0.0203 | Average test loss: 0.0118\n",
      "Step 190: Average train loss: 0.0203 | Average test loss: 0.0118\n",
      "Step 195: Average train loss: 0.0202 | Average test loss: 0.0118\n",
      "Best Epoch: 193\n",
      "Step 0: Average train loss: 0.1067 | Average test loss: 0.0493\n",
      "Step 5: Average train loss: 0.0247 | Average test loss: 0.0188\n",
      "Step 10: Average train loss: 0.0235 | Average test loss: 0.0160\n",
      "Step 15: Average train loss: 0.0230 | Average test loss: 0.0155\n",
      "Step 20: Average train loss: 0.0225 | Average test loss: 0.0142\n",
      "Step 25: Average train loss: 0.0216 | Average test loss: 0.0118\n",
      "Step 30: Average train loss: 0.0214 | Average test loss: 0.0114\n",
      "Step 35: Average train loss: 0.0212 | Average test loss: 0.0114\n",
      "Step 40: Average train loss: 0.0214 | Average test loss: 0.0112\n",
      "Step 45: Average train loss: 0.0215 | Average test loss: 0.0112\n",
      "Step 50: Average train loss: 0.0212 | Average test loss: 0.0111\n",
      "Step 55: Average train loss: 0.0212 | Average test loss: 0.0111\n",
      "Step 60: Average train loss: 0.0212 | Average test loss: 0.0108\n",
      "Step 65: Average train loss: 0.0210 | Average test loss: 0.0109\n",
      "Step 70: Average train loss: 0.0211 | Average test loss: 0.0107\n",
      "Step 75: Average train loss: 0.0211 | Average test loss: 0.0106\n",
      "Step 80: Average train loss: 0.0208 | Average test loss: 0.0103\n",
      "Step 85: Average train loss: 0.0207 | Average test loss: 0.0102\n",
      "Step 90: Average train loss: 0.0209 | Average test loss: 0.0103\n",
      "Step 95: Average train loss: 0.0207 | Average test loss: 0.0102\n",
      "Step 100: Average train loss: 0.0211 | Average test loss: 0.0101\n",
      "Step 105: Average train loss: 0.0208 | Average test loss: 0.0099\n",
      "Step 110: Average train loss: 0.0206 | Average test loss: 0.0097\n",
      "Step 115: Average train loss: 0.0204 | Average test loss: 0.0098\n",
      "Step 120: Average train loss: 0.0205 | Average test loss: 0.0097\n",
      "Step 125: Average train loss: 0.0206 | Average test loss: 0.0096\n",
      "Step 130: Average train loss: 0.0204 | Average test loss: 0.0096\n",
      "Step 135: Average train loss: 0.0201 | Average test loss: 0.0095\n",
      "Step 140: Average train loss: 0.0200 | Average test loss: 0.0094\n",
      "Step 145: Average train loss: 0.0201 | Average test loss: 0.0096\n",
      "Step 150: Average train loss: 0.0200 | Average test loss: 0.0096\n",
      "Step 155: Average train loss: 0.0200 | Average test loss: 0.0092\n",
      "Step 160: Average train loss: 0.0200 | Average test loss: 0.0093\n",
      "Step 165: Average train loss: 0.0202 | Average test loss: 0.0095\n",
      "Step 170: Average train loss: 0.0202 | Average test loss: 0.0096\n",
      "Step 175: Average train loss: 0.0199 | Average test loss: 0.0096\n",
      "Step 180: Average train loss: 0.0198 | Average test loss: 0.0094\n",
      "Step 185: Average train loss: 0.0201 | Average test loss: 0.0093\n",
      "Step 190: Average train loss: 0.0198 | Average test loss: 0.0092\n",
      "Step 195: Average train loss: 0.0198 | Average test loss: 0.0092\n",
      "Best Epoch: 193\n",
      "Step 0: Average train loss: 0.0898 | Average test loss: 0.0396\n",
      "Step 5: Average train loss: 0.0243 | Average test loss: 0.0188\n",
      "Step 10: Average train loss: 0.0241 | Average test loss: 0.0174\n",
      "Step 15: Average train loss: 0.0236 | Average test loss: 0.0171\n",
      "Step 20: Average train loss: 0.0229 | Average test loss: 0.0159\n",
      "Step 25: Average train loss: 0.0218 | Average test loss: 0.0139\n",
      "Step 30: Average train loss: 0.0221 | Average test loss: 0.0127\n",
      "Step 35: Average train loss: 0.0215 | Average test loss: 0.0128\n",
      "Step 40: Average train loss: 0.0214 | Average test loss: 0.0125\n",
      "Step 45: Average train loss: 0.0213 | Average test loss: 0.0123\n",
      "Step 50: Average train loss: 0.0213 | Average test loss: 0.0123\n",
      "Step 55: Average train loss: 0.0212 | Average test loss: 0.0122\n",
      "Step 60: Average train loss: 0.0211 | Average test loss: 0.0121\n",
      "Step 65: Average train loss: 0.0212 | Average test loss: 0.0118\n",
      "Step 70: Average train loss: 0.0210 | Average test loss: 0.0119\n",
      "Step 75: Average train loss: 0.0214 | Average test loss: 0.0118\n",
      "Step 80: Average train loss: 0.0210 | Average test loss: 0.0116\n",
      "Step 85: Average train loss: 0.0208 | Average test loss: 0.0114\n",
      "Step 90: Average train loss: 0.0207 | Average test loss: 0.0114\n",
      "Step 95: Average train loss: 0.0206 | Average test loss: 0.0115\n",
      "Step 100: Average train loss: 0.0204 | Average test loss: 0.0111\n",
      "Step 105: Average train loss: 0.0206 | Average test loss: 0.0111\n",
      "Step 110: Average train loss: 0.0205 | Average test loss: 0.0110\n",
      "Step 115: Average train loss: 0.0203 | Average test loss: 0.0108\n",
      "Step 120: Average train loss: 0.0204 | Average test loss: 0.0108\n",
      "Step 125: Average train loss: 0.0203 | Average test loss: 0.0112\n",
      "Step 130: Average train loss: 0.0202 | Average test loss: 0.0111\n",
      "Step 135: Average train loss: 0.0201 | Average test loss: 0.0110\n",
      "Step 140: Average train loss: 0.0202 | Average test loss: 0.0110\n",
      "Step 145: Average train loss: 0.0199 | Average test loss: 0.0109\n",
      "Step 150: Average train loss: 0.0199 | Average test loss: 0.0110\n",
      "Step 155: Average train loss: 0.0199 | Average test loss: 0.0111\n",
      "Step 160: Average train loss: 0.0200 | Average test loss: 0.0109\n",
      "Step 165: Average train loss: 0.0201 | Average test loss: 0.0109\n",
      "Step 170: Average train loss: 0.0200 | Average test loss: 0.0107\n",
      "Step 175: Average train loss: 0.0197 | Average test loss: 0.0106\n",
      "Step 180: Average train loss: 0.0197 | Average test loss: 0.0106\n",
      "Step 185: Average train loss: 0.0197 | Average test loss: 0.0105\n",
      "Step 190: Average train loss: 0.0198 | Average test loss: 0.0107\n",
      "Step 195: Average train loss: 0.0199 | Average test loss: 0.0107\n",
      "Best Epoch: 185\n"
     ]
    }
   ],
   "source": [
    "# Keep track of the best performing iteration\n",
    "target_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Initialize the trainer\n",
    "    training = Training(target_lstm_list[i+1], X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    target_lstm_list[i+1].load_state_dict(state_dict_list[best_epoch])\n",
    "    target_best_epochs.append(best_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:43:17.998496500Z",
     "start_time": "2025-03-12T17:41:53.765670800Z"
    }
   },
   "id": "a707dd96e1a14b7f"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Maintain lists with all three evaluation metrics used in the paper\n",
    "target_RMSEs = []\n",
    "target_MBEs = []\n",
    "target_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "forecasts = target_lstm_list[0](X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "target_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = target_lstm_list[i+1](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Append the evaluation metrics\n",
    "    target_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    target_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    target_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:43:26.836967500Z",
     "start_time": "2025-03-12T17:43:17.998496500Z"
    }
   },
   "id": "de0660e76f1cde60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Transfer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1aa75c7d725d84"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We freeze the weights and biases of the first layer\n",
    "freezing = []\n",
    "\n",
    "for name, _ in my_lstm.lstm.named_parameters():\n",
    "    freezing.append(name)\n",
    "    \n",
    "freezing = freezing[:4]\n",
    "freezing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:43:26.848368100Z",
     "start_time": "2025-03-12T17:43:26.837967100Z"
    }
   },
   "id": "5a1ee9bd9bff6639"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Average train loss: 0.0140 | Average test loss: 0.0111\n",
      "Step 5: Average train loss: 0.0137 | Average test loss: 0.0106\n",
      "Step 10: Average train loss: 0.0131 | Average test loss: 0.0101\n",
      "Step 15: Average train loss: 0.0127 | Average test loss: 0.0098\n",
      "Step 20: Average train loss: 0.0121 | Average test loss: 0.0094\n",
      "Step 25: Average train loss: 0.0115 | Average test loss: 0.0092\n",
      "Step 30: Average train loss: 0.0116 | Average test loss: 0.0089\n",
      "Step 35: Average train loss: 0.0114 | Average test loss: 0.0087\n",
      "Step 40: Average train loss: 0.0111 | Average test loss: 0.0085\n",
      "Step 45: Average train loss: 0.0110 | Average test loss: 0.0084\n",
      "Step 50: Average train loss: 0.0108 | Average test loss: 0.0082\n",
      "Step 55: Average train loss: 0.0109 | Average test loss: 0.0081\n",
      "Step 60: Average train loss: 0.0107 | Average test loss: 0.0080\n",
      "Step 65: Average train loss: 0.0104 | Average test loss: 0.0080\n",
      "Step 70: Average train loss: 0.0097 | Average test loss: 0.0079\n",
      "Step 75: Average train loss: 0.0100 | Average test loss: 0.0079\n",
      "Step 80: Average train loss: 0.0097 | Average test loss: 0.0078\n",
      "Step 85: Average train loss: 0.0101 | Average test loss: 0.0078\n",
      "Step 90: Average train loss: 0.0095 | Average test loss: 0.0077\n",
      "Step 95: Average train loss: 0.0099 | Average test loss: 0.0077\n",
      "Step 100: Average train loss: 0.0094 | Average test loss: 0.0077\n",
      "Step 105: Average train loss: 0.0100 | Average test loss: 0.0077\n",
      "Step 110: Average train loss: 0.0096 | Average test loss: 0.0077\n",
      "Step 115: Average train loss: 0.0094 | Average test loss: 0.0077\n",
      "Step 120: Average train loss: 0.0096 | Average test loss: 0.0077\n",
      "Step 125: Average train loss: 0.0090 | Average test loss: 0.0077\n",
      "Step 130: Average train loss: 0.0097 | Average test loss: 0.0077\n",
      "Step 135: Average train loss: 0.0091 | Average test loss: 0.0077\n",
      "Step 140: Average train loss: 0.0087 | Average test loss: 0.0077\n",
      "Step 145: Average train loss: 0.0089 | Average test loss: 0.0077\n",
      "Step 150: Average train loss: 0.0091 | Average test loss: 0.0077\n",
      "Step 155: Average train loss: 0.0095 | Average test loss: 0.0077\n",
      "Step 160: Average train loss: 0.0091 | Average test loss: 0.0077\n",
      "Step 165: Average train loss: 0.0092 | Average test loss: 0.0077\n",
      "Step 170: Average train loss: 0.0085 | Average test loss: 0.0077\n",
      "Step 175: Average train loss: 0.0089 | Average test loss: 0.0077\n",
      "Step 180: Average train loss: 0.0090 | Average test loss: 0.0077\n",
      "Step 185: Average train loss: 0.0087 | Average test loss: 0.0077\n",
      "Step 190: Average train loss: 0.0094 | Average test loss: 0.0077\n",
      "Step 195: Average train loss: 0.0094 | Average test loss: 0.0077\n",
      "Best Epoch: 129\n",
      "Step 0: Average train loss: 0.0126 | Average test loss: 0.0112\n",
      "Step 5: Average train loss: 0.0117 | Average test loss: 0.0104\n",
      "Step 10: Average train loss: 0.0110 | Average test loss: 0.0099\n",
      "Step 15: Average train loss: 0.0110 | Average test loss: 0.0094\n",
      "Step 20: Average train loss: 0.0103 | Average test loss: 0.0090\n",
      "Step 25: Average train loss: 0.0100 | Average test loss: 0.0087\n",
      "Step 30: Average train loss: 0.0102 | Average test loss: 0.0085\n",
      "Step 35: Average train loss: 0.0099 | Average test loss: 0.0083\n",
      "Step 40: Average train loss: 0.0097 | Average test loss: 0.0082\n",
      "Step 45: Average train loss: 0.0097 | Average test loss: 0.0081\n",
      "Step 50: Average train loss: 0.0097 | Average test loss: 0.0080\n",
      "Step 55: Average train loss: 0.0095 | Average test loss: 0.0079\n",
      "Step 60: Average train loss: 0.0095 | Average test loss: 0.0079\n",
      "Step 65: Average train loss: 0.0095 | Average test loss: 0.0078\n",
      "Step 70: Average train loss: 0.0093 | Average test loss: 0.0078\n",
      "Step 75: Average train loss: 0.0092 | Average test loss: 0.0077\n",
      "Step 80: Average train loss: 0.0091 | Average test loss: 0.0077\n",
      "Step 85: Average train loss: 0.0088 | Average test loss: 0.0077\n",
      "Step 90: Average train loss: 0.0091 | Average test loss: 0.0077\n",
      "Step 95: Average train loss: 0.0089 | Average test loss: 0.0077\n",
      "Step 100: Average train loss: 0.0088 | Average test loss: 0.0077\n",
      "Step 105: Average train loss: 0.0093 | Average test loss: 0.0076\n",
      "Step 110: Average train loss: 0.0093 | Average test loss: 0.0076\n",
      "Step 115: Average train loss: 0.0088 | Average test loss: 0.0076\n",
      "Step 120: Average train loss: 0.0088 | Average test loss: 0.0076\n",
      "Step 125: Average train loss: 0.0089 | Average test loss: 0.0076\n",
      "Step 130: Average train loss: 0.0087 | Average test loss: 0.0076\n",
      "Step 135: Average train loss: 0.0084 | Average test loss: 0.0076\n",
      "Step 140: Average train loss: 0.0088 | Average test loss: 0.0076\n",
      "Step 145: Average train loss: 0.0088 | Average test loss: 0.0076\n",
      "Step 150: Average train loss: 0.0086 | Average test loss: 0.0076\n",
      "Step 155: Average train loss: 0.0085 | Average test loss: 0.0076\n",
      "Step 160: Average train loss: 0.0090 | Average test loss: 0.0076\n",
      "Step 165: Average train loss: 0.0086 | Average test loss: 0.0076\n",
      "Step 170: Average train loss: 0.0087 | Average test loss: 0.0076\n",
      "Step 175: Average train loss: 0.0089 | Average test loss: 0.0076\n",
      "Step 180: Average train loss: 0.0087 | Average test loss: 0.0076\n",
      "Step 185: Average train loss: 0.0087 | Average test loss: 0.0076\n",
      "Step 190: Average train loss: 0.0088 | Average test loss: 0.0076\n",
      "Step 195: Average train loss: 0.0083 | Average test loss: 0.0076\n",
      "Best Epoch: 187\n",
      "Step 0: Average train loss: 0.0123 | Average test loss: 0.0123\n",
      "Step 5: Average train loss: 0.0116 | Average test loss: 0.0117\n",
      "Step 10: Average train loss: 0.0107 | Average test loss: 0.0113\n",
      "Step 15: Average train loss: 0.0104 | Average test loss: 0.0109\n",
      "Step 20: Average train loss: 0.0104 | Average test loss: 0.0108\n",
      "Step 25: Average train loss: 0.0098 | Average test loss: 0.0107\n",
      "Step 30: Average train loss: 0.0101 | Average test loss: 0.0106\n",
      "Step 35: Average train loss: 0.0096 | Average test loss: 0.0105\n",
      "Step 40: Average train loss: 0.0099 | Average test loss: 0.0105\n",
      "Step 45: Average train loss: 0.0094 | Average test loss: 0.0105\n",
      "Step 50: Average train loss: 0.0099 | Average test loss: 0.0104\n",
      "Step 55: Average train loss: 0.0093 | Average test loss: 0.0104\n",
      "Step 60: Average train loss: 0.0098 | Average test loss: 0.0104\n",
      "Step 65: Average train loss: 0.0092 | Average test loss: 0.0105\n",
      "Step 70: Average train loss: 0.0095 | Average test loss: 0.0105\n",
      "Step 75: Average train loss: 0.0095 | Average test loss: 0.0104\n",
      "Step 80: Average train loss: 0.0095 | Average test loss: 0.0105\n",
      "Step 85: Average train loss: 0.0094 | Average test loss: 0.0106\n",
      "Step 90: Average train loss: 0.0096 | Average test loss: 0.0105\n",
      "Step 95: Average train loss: 0.0098 | Average test loss: 0.0105\n",
      "Step 100: Average train loss: 0.0097 | Average test loss: 0.0104\n",
      "Step 105: Average train loss: 0.0093 | Average test loss: 0.0105\n",
      "Step 110: Average train loss: 0.0093 | Average test loss: 0.0104\n",
      "Step 115: Average train loss: 0.0089 | Average test loss: 0.0104\n",
      "Step 120: Average train loss: 0.0094 | Average test loss: 0.0106\n",
      "Step 125: Average train loss: 0.0091 | Average test loss: 0.0107\n",
      "Step 130: Average train loss: 0.0092 | Average test loss: 0.0106\n",
      "Step 135: Average train loss: 0.0092 | Average test loss: 0.0107\n",
      "Step 140: Average train loss: 0.0091 | Average test loss: 0.0105\n",
      "Step 145: Average train loss: 0.0092 | Average test loss: 0.0105\n",
      "Step 150: Average train loss: 0.0090 | Average test loss: 0.0107\n",
      "Step 155: Average train loss: 0.0092 | Average test loss: 0.0108\n",
      "Step 160: Average train loss: 0.0093 | Average test loss: 0.0105\n",
      "Step 165: Average train loss: 0.0090 | Average test loss: 0.0105\n",
      "Step 170: Average train loss: 0.0093 | Average test loss: 0.0106\n",
      "Step 175: Average train loss: 0.0090 | Average test loss: 0.0106\n",
      "Step 180: Average train loss: 0.0089 | Average test loss: 0.0104\n",
      "Step 185: Average train loss: 0.0089 | Average test loss: 0.0107\n",
      "Step 190: Average train loss: 0.0088 | Average test loss: 0.0106\n",
      "Step 195: Average train loss: 0.0091 | Average test loss: 0.0104\n",
      "Best Epoch: 173\n",
      "Step 0: Average train loss: 0.0140 | Average test loss: 0.0129\n",
      "Step 5: Average train loss: 0.0128 | Average test loss: 0.0119\n",
      "Step 10: Average train loss: 0.0124 | Average test loss: 0.0112\n",
      "Step 15: Average train loss: 0.0120 | Average test loss: 0.0108\n",
      "Step 20: Average train loss: 0.0115 | Average test loss: 0.0106\n",
      "Step 25: Average train loss: 0.0113 | Average test loss: 0.0105\n",
      "Step 30: Average train loss: 0.0113 | Average test loss: 0.0104\n",
      "Step 35: Average train loss: 0.0113 | Average test loss: 0.0103\n",
      "Step 40: Average train loss: 0.0111 | Average test loss: 0.0103\n",
      "Step 45: Average train loss: 0.0111 | Average test loss: 0.0103\n",
      "Step 50: Average train loss: 0.0110 | Average test loss: 0.0103\n",
      "Step 55: Average train loss: 0.0108 | Average test loss: 0.0103\n",
      "Step 60: Average train loss: 0.0107 | Average test loss: 0.0103\n",
      "Step 65: Average train loss: 0.0108 | Average test loss: 0.0103\n",
      "Step 70: Average train loss: 0.0105 | Average test loss: 0.0103\n",
      "Step 75: Average train loss: 0.0109 | Average test loss: 0.0103\n",
      "Step 80: Average train loss: 0.0106 | Average test loss: 0.0104\n",
      "Step 85: Average train loss: 0.0104 | Average test loss: 0.0104\n",
      "Step 90: Average train loss: 0.0104 | Average test loss: 0.0104\n",
      "Step 95: Average train loss: 0.0103 | Average test loss: 0.0104\n",
      "Step 100: Average train loss: 0.0105 | Average test loss: 0.0104\n",
      "Step 105: Average train loss: 0.0105 | Average test loss: 0.0104\n",
      "Step 110: Average train loss: 0.0104 | Average test loss: 0.0104\n",
      "Step 115: Average train loss: 0.0104 | Average test loss: 0.0104\n",
      "Step 120: Average train loss: 0.0104 | Average test loss: 0.0105\n",
      "Step 125: Average train loss: 0.0104 | Average test loss: 0.0105\n",
      "Step 130: Average train loss: 0.0100 | Average test loss: 0.0105\n",
      "Step 135: Average train loss: 0.0100 | Average test loss: 0.0105\n",
      "Step 140: Average train loss: 0.0101 | Average test loss: 0.0105\n",
      "Step 145: Average train loss: 0.0101 | Average test loss: 0.0105\n",
      "Step 150: Average train loss: 0.0100 | Average test loss: 0.0105\n",
      "Step 155: Average train loss: 0.0102 | Average test loss: 0.0105\n",
      "Step 160: Average train loss: 0.0100 | Average test loss: 0.0106\n",
      "Step 165: Average train loss: 0.0100 | Average test loss: 0.0106\n",
      "Step 170: Average train loss: 0.0100 | Average test loss: 0.0106\n",
      "Step 175: Average train loss: 0.0102 | Average test loss: 0.0106\n",
      "Step 180: Average train loss: 0.0101 | Average test loss: 0.0106\n",
      "Step 185: Average train loss: 0.0099 | Average test loss: 0.0106\n",
      "Step 190: Average train loss: 0.0100 | Average test loss: 0.0106\n",
      "Step 195: Average train loss: 0.0100 | Average test loss: 0.0106\n",
      "Best Epoch: 47\n",
      "Step 0: Average train loss: 0.0192 | Average test loss: 0.0132\n",
      "Step 5: Average train loss: 0.0177 | Average test loss: 0.0123\n",
      "Step 10: Average train loss: 0.0167 | Average test loss: 0.0118\n",
      "Step 15: Average train loss: 0.0159 | Average test loss: 0.0116\n",
      "Step 20: Average train loss: 0.0159 | Average test loss: 0.0115\n",
      "Step 25: Average train loss: 0.0156 | Average test loss: 0.0115\n",
      "Step 30: Average train loss: 0.0155 | Average test loss: 0.0115\n",
      "Step 35: Average train loss: 0.0151 | Average test loss: 0.0116\n",
      "Step 40: Average train loss: 0.0152 | Average test loss: 0.0116\n",
      "Step 45: Average train loss: 0.0152 | Average test loss: 0.0117\n",
      "Step 50: Average train loss: 0.0151 | Average test loss: 0.0117\n",
      "Step 55: Average train loss: 0.0149 | Average test loss: 0.0118\n",
      "Step 60: Average train loss: 0.0148 | Average test loss: 0.0118\n",
      "Step 65: Average train loss: 0.0148 | Average test loss: 0.0118\n",
      "Step 70: Average train loss: 0.0149 | Average test loss: 0.0119\n",
      "Step 75: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 80: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 85: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 90: Average train loss: 0.0148 | Average test loss: 0.0119\n",
      "Step 95: Average train loss: 0.0147 | Average test loss: 0.0120\n",
      "Step 100: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 105: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 110: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 115: Average train loss: 0.0146 | Average test loss: 0.0119\n",
      "Step 120: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 125: Average train loss: 0.0146 | Average test loss: 0.0119\n",
      "Step 130: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 135: Average train loss: 0.0147 | Average test loss: 0.0119\n",
      "Step 140: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 145: Average train loss: 0.0146 | Average test loss: 0.0119\n",
      "Step 150: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 155: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 160: Average train loss: 0.0146 | Average test loss: 0.0119\n",
      "Step 165: Average train loss: 0.0146 | Average test loss: 0.0119\n",
      "Step 170: Average train loss: 0.0143 | Average test loss: 0.0119\n",
      "Step 175: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 180: Average train loss: 0.0144 | Average test loss: 0.0119\n",
      "Step 185: Average train loss: 0.0143 | Average test loss: 0.0119\n",
      "Step 190: Average train loss: 0.0145 | Average test loss: 0.0119\n",
      "Step 195: Average train loss: 0.0143 | Average test loss: 0.0119\n",
      "Best Epoch: 25\n",
      "Step 0: Average train loss: 0.0213 | Average test loss: 0.0092\n",
      "Step 5: Average train loss: 0.0193 | Average test loss: 0.0083\n",
      "Step 10: Average train loss: 0.0181 | Average test loss: 0.0078\n",
      "Step 15: Average train loss: 0.0175 | Average test loss: 0.0075\n",
      "Step 20: Average train loss: 0.0170 | Average test loss: 0.0074\n",
      "Step 25: Average train loss: 0.0167 | Average test loss: 0.0074\n",
      "Step 30: Average train loss: 0.0169 | Average test loss: 0.0073\n",
      "Step 35: Average train loss: 0.0166 | Average test loss: 0.0074\n",
      "Step 40: Average train loss: 0.0166 | Average test loss: 0.0074\n",
      "Step 45: Average train loss: 0.0163 | Average test loss: 0.0074\n",
      "Step 50: Average train loss: 0.0163 | Average test loss: 0.0074\n",
      "Step 55: Average train loss: 0.0165 | Average test loss: 0.0075\n",
      "Step 60: Average train loss: 0.0161 | Average test loss: 0.0075\n",
      "Step 65: Average train loss: 0.0163 | Average test loss: 0.0075\n",
      "Step 70: Average train loss: 0.0162 | Average test loss: 0.0076\n",
      "Step 75: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 80: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 85: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 90: Average train loss: 0.0162 | Average test loss: 0.0076\n",
      "Step 95: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 100: Average train loss: 0.0160 | Average test loss: 0.0076\n",
      "Step 105: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 110: Average train loss: 0.0161 | Average test loss: 0.0076\n",
      "Step 115: Average train loss: 0.0160 | Average test loss: 0.0076\n",
      "Step 120: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 125: Average train loss: 0.0162 | Average test loss: 0.0076\n",
      "Step 130: Average train loss: 0.0158 | Average test loss: 0.0076\n",
      "Step 135: Average train loss: 0.0160 | Average test loss: 0.0076\n",
      "Step 140: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 145: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Step 150: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Step 155: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 160: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 165: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 170: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Step 175: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Step 180: Average train loss: 0.0158 | Average test loss: 0.0076\n",
      "Step 185: Average train loss: 0.0159 | Average test loss: 0.0076\n",
      "Step 190: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Step 195: Average train loss: 0.0157 | Average test loss: 0.0076\n",
      "Best Epoch: 31\n",
      "Step 0: Average train loss: 0.0224 | Average test loss: 0.0123\n",
      "Step 5: Average train loss: 0.0196 | Average test loss: 0.0110\n",
      "Step 10: Average train loss: 0.0186 | Average test loss: 0.0105\n",
      "Step 15: Average train loss: 0.0177 | Average test loss: 0.0103\n",
      "Step 20: Average train loss: 0.0175 | Average test loss: 0.0102\n",
      "Step 25: Average train loss: 0.0170 | Average test loss: 0.0102\n",
      "Step 30: Average train loss: 0.0172 | Average test loss: 0.0103\n",
      "Step 35: Average train loss: 0.0170 | Average test loss: 0.0103\n",
      "Step 40: Average train loss: 0.0167 | Average test loss: 0.0104\n",
      "Step 45: Average train loss: 0.0168 | Average test loss: 0.0105\n",
      "Step 50: Average train loss: 0.0166 | Average test loss: 0.0106\n",
      "Step 55: Average train loss: 0.0164 | Average test loss: 0.0107\n",
      "Step 60: Average train loss: 0.0166 | Average test loss: 0.0107\n",
      "Step 65: Average train loss: 0.0164 | Average test loss: 0.0108\n",
      "Step 70: Average train loss: 0.0163 | Average test loss: 0.0108\n",
      "Step 75: Average train loss: 0.0164 | Average test loss: 0.0109\n",
      "Step 80: Average train loss: 0.0162 | Average test loss: 0.0109\n",
      "Step 85: Average train loss: 0.0162 | Average test loss: 0.0109\n",
      "Step 90: Average train loss: 0.0162 | Average test loss: 0.0110\n",
      "Step 95: Average train loss: 0.0162 | Average test loss: 0.0110\n",
      "Step 100: Average train loss: 0.0163 | Average test loss: 0.0110\n",
      "Step 105: Average train loss: 0.0161 | Average test loss: 0.0111\n",
      "Step 110: Average train loss: 0.0163 | Average test loss: 0.0111\n",
      "Step 115: Average train loss: 0.0162 | Average test loss: 0.0111\n",
      "Step 120: Average train loss: 0.0162 | Average test loss: 0.0112\n",
      "Step 125: Average train loss: 0.0162 | Average test loss: 0.0112\n",
      "Step 130: Average train loss: 0.0162 | Average test loss: 0.0112\n",
      "Step 135: Average train loss: 0.0161 | Average test loss: 0.0112\n",
      "Step 140: Average train loss: 0.0160 | Average test loss: 0.0112\n",
      "Step 145: Average train loss: 0.0159 | Average test loss: 0.0113\n",
      "Step 150: Average train loss: 0.0162 | Average test loss: 0.0113\n",
      "Step 155: Average train loss: 0.0161 | Average test loss: 0.0113\n",
      "Step 160: Average train loss: 0.0161 | Average test loss: 0.0113\n",
      "Step 165: Average train loss: 0.0160 | Average test loss: 0.0114\n",
      "Step 170: Average train loss: 0.0160 | Average test loss: 0.0114\n",
      "Step 175: Average train loss: 0.0160 | Average test loss: 0.0114\n",
      "Step 180: Average train loss: 0.0159 | Average test loss: 0.0114\n",
      "Step 185: Average train loss: 0.0159 | Average test loss: 0.0114\n",
      "Step 190: Average train loss: 0.0160 | Average test loss: 0.0114\n",
      "Step 195: Average train loss: 0.0160 | Average test loss: 0.0114\n",
      "Best Epoch: 21\n",
      "Step 0: Average train loss: 0.0224 | Average test loss: 0.0118\n",
      "Step 5: Average train loss: 0.0201 | Average test loss: 0.0106\n",
      "Step 10: Average train loss: 0.0187 | Average test loss: 0.0101\n",
      "Step 15: Average train loss: 0.0184 | Average test loss: 0.0100\n",
      "Step 20: Average train loss: 0.0180 | Average test loss: 0.0100\n",
      "Step 25: Average train loss: 0.0179 | Average test loss: 0.0100\n",
      "Step 30: Average train loss: 0.0178 | Average test loss: 0.0101\n",
      "Step 35: Average train loss: 0.0178 | Average test loss: 0.0101\n",
      "Step 40: Average train loss: 0.0177 | Average test loss: 0.0102\n",
      "Step 45: Average train loss: 0.0176 | Average test loss: 0.0102\n",
      "Step 50: Average train loss: 0.0175 | Average test loss: 0.0103\n",
      "Step 55: Average train loss: 0.0175 | Average test loss: 0.0103\n",
      "Step 60: Average train loss: 0.0174 | Average test loss: 0.0104\n",
      "Step 65: Average train loss: 0.0175 | Average test loss: 0.0104\n",
      "Step 70: Average train loss: 0.0173 | Average test loss: 0.0105\n",
      "Step 75: Average train loss: 0.0174 | Average test loss: 0.0105\n",
      "Step 80: Average train loss: 0.0174 | Average test loss: 0.0105\n",
      "Step 85: Average train loss: 0.0174 | Average test loss: 0.0106\n",
      "Step 90: Average train loss: 0.0174 | Average test loss: 0.0106\n",
      "Step 95: Average train loss: 0.0174 | Average test loss: 0.0107\n",
      "Step 100: Average train loss: 0.0171 | Average test loss: 0.0107\n",
      "Step 105: Average train loss: 0.0172 | Average test loss: 0.0107\n",
      "Step 110: Average train loss: 0.0172 | Average test loss: 0.0107\n",
      "Step 115: Average train loss: 0.0170 | Average test loss: 0.0107\n",
      "Step 120: Average train loss: 0.0172 | Average test loss: 0.0108\n",
      "Step 125: Average train loss: 0.0171 | Average test loss: 0.0108\n",
      "Step 130: Average train loss: 0.0171 | Average test loss: 0.0108\n",
      "Step 135: Average train loss: 0.0172 | Average test loss: 0.0108\n",
      "Step 140: Average train loss: 0.0171 | Average test loss: 0.0108\n",
      "Step 145: Average train loss: 0.0170 | Average test loss: 0.0109\n",
      "Step 150: Average train loss: 0.0170 | Average test loss: 0.0109\n",
      "Step 155: Average train loss: 0.0171 | Average test loss: 0.0109\n",
      "Step 160: Average train loss: 0.0170 | Average test loss: 0.0109\n",
      "Step 165: Average train loss: 0.0170 | Average test loss: 0.0109\n",
      "Step 170: Average train loss: 0.0169 | Average test loss: 0.0110\n",
      "Step 175: Average train loss: 0.0170 | Average test loss: 0.0110\n",
      "Step 180: Average train loss: 0.0169 | Average test loss: 0.0110\n",
      "Step 185: Average train loss: 0.0168 | Average test loss: 0.0109\n",
      "Step 190: Average train loss: 0.0169 | Average test loss: 0.0110\n",
      "Step 195: Average train loss: 0.0168 | Average test loss: 0.0110\n",
      "Best Epoch: 19\n",
      "Step 0: Average train loss: 0.0248 | Average test loss: 0.0116\n",
      "Step 5: Average train loss: 0.0218 | Average test loss: 0.0106\n",
      "Step 10: Average train loss: 0.0209 | Average test loss: 0.0102\n",
      "Step 15: Average train loss: 0.0203 | Average test loss: 0.0101\n",
      "Step 20: Average train loss: 0.0202 | Average test loss: 0.0101\n",
      "Step 25: Average train loss: 0.0199 | Average test loss: 0.0102\n",
      "Step 30: Average train loss: 0.0200 | Average test loss: 0.0102\n",
      "Step 35: Average train loss: 0.0198 | Average test loss: 0.0103\n",
      "Step 40: Average train loss: 0.0196 | Average test loss: 0.0103\n",
      "Step 45: Average train loss: 0.0196 | Average test loss: 0.0104\n",
      "Step 50: Average train loss: 0.0197 | Average test loss: 0.0104\n",
      "Step 55: Average train loss: 0.0196 | Average test loss: 0.0104\n",
      "Step 60: Average train loss: 0.0197 | Average test loss: 0.0105\n",
      "Step 65: Average train loss: 0.0197 | Average test loss: 0.0105\n",
      "Step 70: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 75: Average train loss: 0.0196 | Average test loss: 0.0105\n",
      "Step 80: Average train loss: 0.0197 | Average test loss: 0.0105\n",
      "Step 85: Average train loss: 0.0197 | Average test loss: 0.0105\n",
      "Step 90: Average train loss: 0.0196 | Average test loss: 0.0105\n",
      "Step 95: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 100: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 105: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 110: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 115: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 120: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 125: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 130: Average train loss: 0.0192 | Average test loss: 0.0105\n",
      "Step 135: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 140: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 145: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 150: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 155: Average train loss: 0.0193 | Average test loss: 0.0105\n",
      "Step 160: Average train loss: 0.0193 | Average test loss: 0.0105\n",
      "Step 165: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Step 170: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 175: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 180: Average train loss: 0.0193 | Average test loss: 0.0105\n",
      "Step 185: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 190: Average train loss: 0.0194 | Average test loss: 0.0105\n",
      "Step 195: Average train loss: 0.0195 | Average test loss: 0.0105\n",
      "Best Epoch: 18\n",
      "Step 0: Average train loss: 0.0241 | Average test loss: 0.0112\n",
      "Step 5: Average train loss: 0.0219 | Average test loss: 0.0103\n",
      "Step 10: Average train loss: 0.0210 | Average test loss: 0.0100\n",
      "Step 15: Average train loss: 0.0208 | Average test loss: 0.0099\n",
      "Step 20: Average train loss: 0.0206 | Average test loss: 0.0099\n",
      "Step 25: Average train loss: 0.0203 | Average test loss: 0.0099\n",
      "Step 30: Average train loss: 0.0202 | Average test loss: 0.0099\n",
      "Step 35: Average train loss: 0.0203 | Average test loss: 0.0099\n",
      "Step 40: Average train loss: 0.0201 | Average test loss: 0.0099\n",
      "Step 45: Average train loss: 0.0201 | Average test loss: 0.0100\n",
      "Step 50: Average train loss: 0.0201 | Average test loss: 0.0100\n",
      "Step 55: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 60: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 65: Average train loss: 0.0202 | Average test loss: 0.0100\n",
      "Step 70: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 75: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 80: Average train loss: 0.0199 | Average test loss: 0.0100\n",
      "Step 85: Average train loss: 0.0199 | Average test loss: 0.0100\n",
      "Step 90: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 95: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 100: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 105: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 110: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 115: Average train loss: 0.0200 | Average test loss: 0.0100\n",
      "Step 120: Average train loss: 0.0199 | Average test loss: 0.0100\n",
      "Step 125: Average train loss: 0.0198 | Average test loss: 0.0100\n",
      "Step 130: Average train loss: 0.0198 | Average test loss: 0.0100\n",
      "Step 135: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 140: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 145: Average train loss: 0.0199 | Average test loss: 0.0099\n",
      "Step 150: Average train loss: 0.0197 | Average test loss: 0.0099\n",
      "Step 155: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 160: Average train loss: 0.0199 | Average test loss: 0.0099\n",
      "Step 165: Average train loss: 0.0197 | Average test loss: 0.0099\n",
      "Step 170: Average train loss: 0.0197 | Average test loss: 0.0099\n",
      "Step 175: Average train loss: 0.0199 | Average test loss: 0.0099\n",
      "Step 180: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 185: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 190: Average train loss: 0.0198 | Average test loss: 0.0099\n",
      "Step 195: Average train loss: 0.0196 | Average test loss: 0.0099\n",
      "Best Epoch: 19\n",
      "Step 0: Average train loss: 0.0233 | Average test loss: 0.0090\n",
      "Step 5: Average train loss: 0.0210 | Average test loss: 0.0082\n",
      "Step 10: Average train loss: 0.0204 | Average test loss: 0.0079\n",
      "Step 15: Average train loss: 0.0200 | Average test loss: 0.0078\n",
      "Step 20: Average train loss: 0.0198 | Average test loss: 0.0077\n",
      "Step 25: Average train loss: 0.0197 | Average test loss: 0.0077\n",
      "Step 30: Average train loss: 0.0197 | Average test loss: 0.0077\n",
      "Step 35: Average train loss: 0.0195 | Average test loss: 0.0077\n",
      "Step 40: Average train loss: 0.0196 | Average test loss: 0.0077\n",
      "Step 45: Average train loss: 0.0196 | Average test loss: 0.0077\n",
      "Step 50: Average train loss: 0.0193 | Average test loss: 0.0077\n",
      "Step 55: Average train loss: 0.0196 | Average test loss: 0.0077\n",
      "Step 60: Average train loss: 0.0194 | Average test loss: 0.0076\n",
      "Step 65: Average train loss: 0.0196 | Average test loss: 0.0076\n",
      "Step 70: Average train loss: 0.0195 | Average test loss: 0.0076\n",
      "Step 75: Average train loss: 0.0195 | Average test loss: 0.0076\n",
      "Step 80: Average train loss: 0.0194 | Average test loss: 0.0076\n",
      "Step 85: Average train loss: 0.0194 | Average test loss: 0.0076\n",
      "Step 90: Average train loss: 0.0193 | Average test loss: 0.0075\n",
      "Step 95: Average train loss: 0.0194 | Average test loss: 0.0075\n",
      "Step 100: Average train loss: 0.0192 | Average test loss: 0.0075\n",
      "Step 105: Average train loss: 0.0192 | Average test loss: 0.0075\n",
      "Step 110: Average train loss: 0.0193 | Average test loss: 0.0075\n",
      "Step 115: Average train loss: 0.0193 | Average test loss: 0.0075\n",
      "Step 120: Average train loss: 0.0193 | Average test loss: 0.0075\n",
      "Step 125: Average train loss: 0.0192 | Average test loss: 0.0075\n",
      "Step 130: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 135: Average train loss: 0.0193 | Average test loss: 0.0074\n",
      "Step 140: Average train loss: 0.0191 | Average test loss: 0.0074\n",
      "Step 145: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 150: Average train loss: 0.0193 | Average test loss: 0.0074\n",
      "Step 155: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 160: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 165: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 170: Average train loss: 0.0192 | Average test loss: 0.0074\n",
      "Step 175: Average train loss: 0.0193 | Average test loss: 0.0074\n",
      "Step 180: Average train loss: 0.0191 | Average test loss: 0.0074\n",
      "Step 185: Average train loss: 0.0191 | Average test loss: 0.0074\n",
      "Step 190: Average train loss: 0.0191 | Average test loss: 0.0074\n",
      "Step 195: Average train loss: 0.0190 | Average test loss: 0.0074\n",
      "Best Epoch: 192\n",
      "Step 0: Average train loss: 0.0228 | Average test loss: 0.0111\n",
      "Step 5: Average train loss: 0.0204 | Average test loss: 0.0100\n",
      "Step 10: Average train loss: 0.0201 | Average test loss: 0.0097\n",
      "Step 15: Average train loss: 0.0196 | Average test loss: 0.0096\n",
      "Step 20: Average train loss: 0.0196 | Average test loss: 0.0095\n",
      "Step 25: Average train loss: 0.0194 | Average test loss: 0.0095\n",
      "Step 30: Average train loss: 0.0195 | Average test loss: 0.0095\n",
      "Step 35: Average train loss: 0.0190 | Average test loss: 0.0095\n",
      "Step 40: Average train loss: 0.0192 | Average test loss: 0.0095\n",
      "Step 45: Average train loss: 0.0192 | Average test loss: 0.0095\n",
      "Step 50: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 55: Average train loss: 0.0192 | Average test loss: 0.0095\n",
      "Step 60: Average train loss: 0.0189 | Average test loss: 0.0096\n",
      "Step 65: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 70: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 75: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 80: Average train loss: 0.0191 | Average test loss: 0.0096\n",
      "Step 85: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 90: Average train loss: 0.0189 | Average test loss: 0.0096\n",
      "Step 95: Average train loss: 0.0189 | Average test loss: 0.0096\n",
      "Step 100: Average train loss: 0.0190 | Average test loss: 0.0096\n",
      "Step 105: Average train loss: 0.0188 | Average test loss: 0.0097\n",
      "Step 110: Average train loss: 0.0191 | Average test loss: 0.0097\n",
      "Step 115: Average train loss: 0.0190 | Average test loss: 0.0097\n",
      "Step 120: Average train loss: 0.0187 | Average test loss: 0.0098\n",
      "Step 125: Average train loss: 0.0189 | Average test loss: 0.0097\n",
      "Step 130: Average train loss: 0.0189 | Average test loss: 0.0098\n",
      "Step 135: Average train loss: 0.0186 | Average test loss: 0.0098\n",
      "Step 140: Average train loss: 0.0187 | Average test loss: 0.0098\n",
      "Step 145: Average train loss: 0.0188 | Average test loss: 0.0098\n",
      "Step 150: Average train loss: 0.0187 | Average test loss: 0.0099\n",
      "Step 155: Average train loss: 0.0188 | Average test loss: 0.0099\n",
      "Step 160: Average train loss: 0.0187 | Average test loss: 0.0099\n",
      "Step 165: Average train loss: 0.0187 | Average test loss: 0.0099\n",
      "Step 170: Average train loss: 0.0186 | Average test loss: 0.0099\n",
      "Step 175: Average train loss: 0.0188 | Average test loss: 0.0099\n",
      "Step 180: Average train loss: 0.0187 | Average test loss: 0.0100\n",
      "Step 185: Average train loss: 0.0186 | Average test loss: 0.0100\n",
      "Step 190: Average train loss: 0.0185 | Average test loss: 0.0100\n",
      "Step 195: Average train loss: 0.0187 | Average test loss: 0.0100\n",
      "Best Epoch: 26\n"
     ]
    }
   ],
   "source": [
    "transfer_models = []\n",
    "transfer_best_epochs = [0]\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    transfer_model  = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "    transfer_model.load_state_dict(torch.load('../models/AUS/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "       \n",
    "    for name, param in transfer_model.lstm.named_parameters():\n",
    "        if any(freezing_name in name for freezing_name in freezing):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Initialize the trainer\n",
    "    training = Training(transfer_model, \n",
    "                              X_train_target_list[i], y_train_target_list[i], X_test_target_list[i], y_test_target_list[i], \n",
    "                              epochs=epochs, batch_size = batch_size, learning_rate =learning_rate/10)\n",
    "\n",
    "    # Train the model and return the trained parameters and the best iteration\n",
    "    state_dict_list, best_epoch = training.fit()\n",
    "    \n",
    "    # Load the state dictionary of the best performing model\n",
    "    transfer_model.load_state_dict(state_dict_list[best_epoch])\n",
    "    \n",
    "    transfer_best_epochs.append(best_epoch)\n",
    "    transfer_models.append(transfer_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:47.402741Z",
     "start_time": "2025-03-12T17:43:26.848368100Z"
    }
   },
   "id": "350c3b23d316dd6c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "transfer_RMSEs = []\n",
    "transfer_MBEs = []\n",
    "transfer_MAEs = []\n",
    "\n",
    "# Evaluate a clean model\n",
    "\n",
    "transfer_model = LSTM(input_size,hidden_size,num_layers, forecast_period, dropout).to(device)\n",
    "transfer_model.load_state_dict(torch.load('../models/AUS/' + data_name + '/model_' + data_name + '_transfer_0'))\n",
    "\n",
    "forecasts = transfer_model(X_eval_target_list[0].to(device))\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = transfer_models[i](X_eval_target_list[i].to(device))\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    transfer_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    transfer_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    transfer_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:56.204995600Z",
     "start_time": "2025-03-12T17:44:47.401328Z"
    }
   },
   "id": "ef5a423788b3e209"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa98825b51380"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "baseline_RMSEs = []\n",
    "baseline_MBEs = []\n",
    "baseline_MAEs = []\n",
    "\n",
    "# Evaluate a clean model, our forecast in this case is basically the first feature in our features tensor, as we predict the next day to be the previous one \n",
    "forecasts = X_eval_target_list[0][:,:,0]\n",
    "source_eval = Evaluation(y_eval_target_list[0].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])\n",
    "\n",
    "for i in range(len(training_months)):\n",
    "    # Forecast with the model\n",
    "    forecasts = X_eval_target_list[i][:,:,0]\n",
    "    # Evaluate the model performance\n",
    "    source_eval = Evaluation(y_eval_target_list[i].detach().flatten().numpy(), forecasts.cpu().detach().flatten().numpy())\n",
    "\n",
    "    # Show the evaluation metrics\n",
    "    baseline_RMSEs.append(source_eval.metrics()['RMSE'].values[0])\n",
    "    baseline_MBEs.append(source_eval.metrics()['MBE'].values[0])\n",
    "    baseline_MAEs.append(source_eval.metrics()['MAE'].values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.162447600Z",
     "start_time": "2025-03-12T17:44:56.206995500Z"
    }
   },
   "id": "fa9a67df5080bb4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Final visualisation and export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19632e7bcce954a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x237d38f1610>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(target_RMSEs,label='target')\n",
    "plt.plot(transfer_RMSEs,label='transfer')\n",
    "plt.plot(baseline_RMSEs, label='baseline')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.294477700Z",
     "start_time": "2025-03-12T17:44:59.164448400Z"
    }
   },
   "id": "1a796e9ba899736a"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "column_names = []\n",
    "\n",
    "for i in range(len(training_months)+1):\n",
    "    column_names.append(str(i) + 'm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.304839800Z",
     "start_time": "2025-03-12T17:44:59.295477100Z"
    }
   },
   "id": "68f738ffd4e8f96d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "     Baseline RMSE  Target RMSE  Transfer RMSE  Baseline MBE  Target MBE  \\\n0m           0.173        0.337          0.143        -0.000       0.176   \n1m           0.173        0.180          0.152        -0.000       0.069   \n2m           0.173        0.198          0.146        -0.000       0.006   \n3m           0.173        0.158          0.143        -0.000       0.023   \n4m           0.173        0.155          0.144        -0.000       0.042   \n5m           0.173        0.151          0.144        -0.000       0.017   \n6m           0.173        0.152          0.144        -0.000       0.020   \n7m           0.173        0.150          0.145        -0.000       0.021   \n8m           0.173        0.147          0.143        -0.000       0.021   \n9m           0.173        0.149          0.144        -0.000       0.019   \n10m          0.173        0.145          0.141        -0.000       0.012   \n11m          0.173        0.145          0.141        -0.000       0.015   \n12m          0.173        0.144          0.144        -0.000       0.017   \n\n     Transfer MBE  Baseline MAE  Target MAE  Transfer MAE  Target epoch  \\\n0m         -0.005         0.077       0.195         0.079             0   \n1m          0.035         0.077       0.098         0.081            28   \n2m          0.026         0.077       0.100         0.078           123   \n3m          0.017         0.077       0.082         0.075            19   \n4m          0.023         0.077       0.084         0.077           137   \n5m          0.013         0.077       0.084         0.078             5   \n6m          0.012         0.077       0.084         0.078           188   \n7m          0.017         0.077       0.082         0.079           198   \n8m          0.015         0.077       0.080         0.077           184   \n9m          0.013         0.077       0.083         0.079           197   \n10m         0.011         0.077       0.080         0.076           193   \n11m         0.014         0.077       0.079         0.076           193   \n12m         0.017         0.077       0.079         0.077           185   \n\n     Transfer epoch  \n0m                0  \n1m              129  \n2m              187  \n3m              173  \n4m               47  \n5m               25  \n6m               31  \n7m               21  \n8m               19  \n9m               18  \n10m              19  \n11m             192  \n12m              26  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Baseline RMSE</th>\n      <th>Target RMSE</th>\n      <th>Transfer RMSE</th>\n      <th>Baseline MBE</th>\n      <th>Target MBE</th>\n      <th>Transfer MBE</th>\n      <th>Baseline MAE</th>\n      <th>Target MAE</th>\n      <th>Transfer MAE</th>\n      <th>Target epoch</th>\n      <th>Transfer epoch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0m</th>\n      <td>0.173</td>\n      <td>0.337</td>\n      <td>0.143</td>\n      <td>-0.000</td>\n      <td>0.176</td>\n      <td>-0.005</td>\n      <td>0.077</td>\n      <td>0.195</td>\n      <td>0.079</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1m</th>\n      <td>0.173</td>\n      <td>0.180</td>\n      <td>0.152</td>\n      <td>-0.000</td>\n      <td>0.069</td>\n      <td>0.035</td>\n      <td>0.077</td>\n      <td>0.098</td>\n      <td>0.081</td>\n      <td>28</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>2m</th>\n      <td>0.173</td>\n      <td>0.198</td>\n      <td>0.146</td>\n      <td>-0.000</td>\n      <td>0.006</td>\n      <td>0.026</td>\n      <td>0.077</td>\n      <td>0.100</td>\n      <td>0.078</td>\n      <td>123</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>3m</th>\n      <td>0.173</td>\n      <td>0.158</td>\n      <td>0.143</td>\n      <td>-0.000</td>\n      <td>0.023</td>\n      <td>0.017</td>\n      <td>0.077</td>\n      <td>0.082</td>\n      <td>0.075</td>\n      <td>19</td>\n      <td>173</td>\n    </tr>\n    <tr>\n      <th>4m</th>\n      <td>0.173</td>\n      <td>0.155</td>\n      <td>0.144</td>\n      <td>-0.000</td>\n      <td>0.042</td>\n      <td>0.023</td>\n      <td>0.077</td>\n      <td>0.084</td>\n      <td>0.077</td>\n      <td>137</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>5m</th>\n      <td>0.173</td>\n      <td>0.151</td>\n      <td>0.144</td>\n      <td>-0.000</td>\n      <td>0.017</td>\n      <td>0.013</td>\n      <td>0.077</td>\n      <td>0.084</td>\n      <td>0.078</td>\n      <td>5</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6m</th>\n      <td>0.173</td>\n      <td>0.152</td>\n      <td>0.144</td>\n      <td>-0.000</td>\n      <td>0.020</td>\n      <td>0.012</td>\n      <td>0.077</td>\n      <td>0.084</td>\n      <td>0.078</td>\n      <td>188</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>7m</th>\n      <td>0.173</td>\n      <td>0.150</td>\n      <td>0.145</td>\n      <td>-0.000</td>\n      <td>0.021</td>\n      <td>0.017</td>\n      <td>0.077</td>\n      <td>0.082</td>\n      <td>0.079</td>\n      <td>198</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8m</th>\n      <td>0.173</td>\n      <td>0.147</td>\n      <td>0.143</td>\n      <td>-0.000</td>\n      <td>0.021</td>\n      <td>0.015</td>\n      <td>0.077</td>\n      <td>0.080</td>\n      <td>0.077</td>\n      <td>184</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>9m</th>\n      <td>0.173</td>\n      <td>0.149</td>\n      <td>0.144</td>\n      <td>-0.000</td>\n      <td>0.019</td>\n      <td>0.013</td>\n      <td>0.077</td>\n      <td>0.083</td>\n      <td>0.079</td>\n      <td>197</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>10m</th>\n      <td>0.173</td>\n      <td>0.145</td>\n      <td>0.141</td>\n      <td>-0.000</td>\n      <td>0.012</td>\n      <td>0.011</td>\n      <td>0.077</td>\n      <td>0.080</td>\n      <td>0.076</td>\n      <td>193</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>11m</th>\n      <td>0.173</td>\n      <td>0.145</td>\n      <td>0.141</td>\n      <td>-0.000</td>\n      <td>0.015</td>\n      <td>0.014</td>\n      <td>0.077</td>\n      <td>0.079</td>\n      <td>0.076</td>\n      <td>193</td>\n      <td>192</td>\n    </tr>\n    <tr>\n      <th>12m</th>\n      <td>0.173</td>\n      <td>0.144</td>\n      <td>0.144</td>\n      <td>-0.000</td>\n      <td>0.017</td>\n      <td>0.017</td>\n      <td>0.077</td>\n      <td>0.079</td>\n      <td>0.077</td>\n      <td>185</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics = pd.DataFrame([baseline_RMSEs, target_RMSEs, transfer_RMSEs,\n",
    "                            baseline_MBEs, target_MBEs, transfer_MBEs,\n",
    "                            baseline_MAEs, target_MAEs, transfer_MAEs, \n",
    "                            target_best_epochs, transfer_best_epochs],\n",
    "                           columns=column_names, index=['Baseline RMSE', 'Target RMSE', 'Transfer RMSE', \n",
    "                                                        'Baseline MBE', 'Target MBE', 'Transfer MBE', \n",
    "                                                        'Baseline MAE', 'Target MAE', 'Transfer MAE', \n",
    "                                                        'Target epoch', 'Transfer epoch']).transpose()\n",
    "\n",
    "all_metrics['Target epoch'] = all_metrics['Target epoch'].astype(int)\n",
    "all_metrics['Transfer epoch'] = all_metrics['Transfer epoch'].astype(int)\n",
    "all_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.319488400Z",
     "start_time": "2025-03-12T17:44:59.301549800Z"
    }
   },
   "id": "74cc8e6ba8e067e2"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "all_metrics.to_csv('../results/AUS/' + 'summary_table_' + data_name + '.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.403478200Z",
     "start_time": "2025-03-12T17:44:59.317487300Z"
    }
   },
   "id": "4e911dfa55c19f92"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'base_australia_1'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:44:59.404478100Z",
     "start_time": "2025-03-12T17:44:59.328346500Z"
    }
   },
   "id": "62ad32304b2e2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
